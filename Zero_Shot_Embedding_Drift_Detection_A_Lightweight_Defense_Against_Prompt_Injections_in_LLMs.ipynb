{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnirudhSekar/ZEDD/blob/main/Zero_Shot_Embedding_Drift_Detection_A_Lightweight_Defense_Against_Prompt_Injections_in_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FddsHwSCzhrB"
      },
      "outputs": [],
      "source": [
        "%autosave 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGhqt-s_kZfU"
      },
      "source": [
        "# Install Necessary Modules and Mount to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oayvNM3yJfCA"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==2.15.0 openai fasttext tqdm numpy==1.25.0 tiktoken transformers==4.44.0 sentence-transformers==3.0.1\n",
        "!rm -rf ~/.cache/huggingface/datasets\n",
        "!rm -rf /root/.cache/huggingface/datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "1GlEQihRAPIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a5rKTtyjdzj"
      },
      "source": [
        "#Load and Filter Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this section processes the `microsoft/llmail-inject-challenge` dataset to clean and prepare it for analysis. It begins by removing duplicate entries based on normalized text content, then filters the dataset to keep only English-language entries (plus some system messages) using Facebook’s FastText language detection model. The code also calculates the average character length of text content across different phases of the dataset. Finally, it splits the cleaned dataset into four equal quarters, creating separate dataset objects for each portion."
      ],
      "metadata": {
        "id": "y1jpHrux_XWX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22MfHNTRkPjR"
      },
      "outputs": [],
      "source": [
        "def calculate_avg_length(dataset):\n",
        "\n",
        "    \"\"\"calculate_avg_length(dataset) calculates the average character length of text content across\n",
        "    two phases of a dataset. It sums the character counts of all \"body\" entries\n",
        "    from both Phase1 and Phase2, then divides by the total number of entries to\n",
        "    return the mean length.\"\"\"\n",
        "\n",
        "    phase1 = dataset[\"Phase1\"]\n",
        "    phase2 = dataset[\"Phase2\"]\n",
        "\n",
        "    phase1_total = sum(len(body.strip()) for body in phase1[\"body\"])\n",
        "    phase2_total = sum(len(body.strip()) for body in phase2[\"body\"])\n",
        "\n",
        "    total_rows = len(phase1[\"body\"]) + len(phase2[\"body\"])\n",
        "    return (phase1_total + phase2_total) / total_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWEdJN1KyJVU"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "client = OpenAI(api_key=userdata.get(\"OPENAI_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clUgYGpYnEha"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "try:\n",
        "    dataset_injected = load_dataset(\"microsoft/llmail-inject-challenge\")\n",
        "except NotImplementedError:\n",
        "    print(\"Loading from cache failed, attempting to force download.\")\n",
        "    dataset_injected = load_dataset(\"microsoft/llmail-inject-challenge\", download_mode=\"force_redownload\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pCAy3MXlMTO"
      },
      "outputs": [],
      "source": [
        "print(calculate_avg_length(dataset_injected))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St6Jj0Lvkqiv"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "\n",
        "def remove_duplicates_from_dataset_dict(dataset_dict):\n",
        "\n",
        "    cleaned_dict = {}\n",
        "    seen = set()\n",
        "\n",
        "    for phase_name, dataset in dataset_dict.items():\n",
        "        print(f\"\\n=== Processing {phase_name} ===\")\n",
        "        print(f\"Original size: {len(dataset)}\")\n",
        "\n",
        "\n",
        "        df = dataset.to_pandas()\n",
        "        df['body_normalized'] = df['body'].str.replace(r'\\s+', '', regex=True).str.lower()\n",
        "\n",
        "        mask = ~df['body_normalized'].isin(seen)\n",
        "        df_filtered = df[mask].copy()\n",
        "\n",
        "\n",
        "        df_deduped = df_filtered.drop_duplicates(subset=['body_normalized'], keep='first')\n",
        "\n",
        "\n",
        "        seen.update(df_deduped['body_normalized'].tolist())\n",
        "\n",
        "\n",
        "        df_deduped = df_deduped.drop('body_normalized', axis=1)\n",
        "\n",
        "        print(f\"After deduplication: {len(df_deduped)} (removed {len(df) - len(df_deduped)} duplicates)\")\n",
        "\n",
        "\n",
        "        cleaned_dict[phase_name] = Dataset.from_pandas(df_deduped)\n",
        "\n",
        "    return DatasetDict(cleaned_dict)\n",
        "\n",
        "dataset_injected = remove_duplicates_from_dataset_dict(dataset_injected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rl_8WuWltxR"
      },
      "outputs": [],
      "source": [
        "print(calculate_avg_length(dataset_injected))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF1UURaodMGK"
      },
      "outputs": [],
      "source": [
        "!wget -q https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n",
        "\n",
        "\n",
        "import fasttext\n",
        "from datasets import DatasetDict\n",
        "from collections import defaultdict\n",
        "\n",
        "ft_model = fasttext.load_model(\"lid.176.ftz\")\n",
        "\n",
        "def fasttext_detect_language(text, threshold=0.05):\n",
        "    try:\n",
        "        if not text or not isinstance(text, str):\n",
        "            return 'unknown'\n",
        "\n",
        "        clean_text = text.strip()\n",
        "        if not clean_text:\n",
        "            return 'unknown'\n",
        "\n",
        "\n",
        "        clean_text = ' '.join(clean_text.split())\n",
        "\n",
        "        prediction = ft_model.predict(clean_text, k=1)\n",
        "        label, prob = prediction[0][0], prediction[1][0]\n",
        "\n",
        "        detected_lang = label.replace('__label__', '')\n",
        "\n",
        "        return detected_lang\n",
        "    except Exception as e:\n",
        "        return 'unknown'\n",
        "\n",
        "\n",
        "def make_filter_fn():\n",
        "    local_counter = defaultdict(int)\n",
        "    def _filter(entry):\n",
        "        body = entry.get(\"body\", \"\")\n",
        "        if not isinstance(body, str) or len(body.strip()) == 0:\n",
        "            local_counter[\"removed\"] += 1\n",
        "            return False\n",
        "        if 'system' in body.lower() or '<<' in body.lower():\n",
        "            local_counter[\"kept_system\"] += 1\n",
        "            return True\n",
        "        lang = fasttext_detect_language(body)\n",
        "        if lang == 'en':\n",
        "            local_counter[\"kept_en\"] += 1\n",
        "            return True\n",
        "        else:\n",
        "            local_counter[\"removed\"] += 1\n",
        "            return False\n",
        "    return _filter, local_counter\n",
        "\n",
        "def filter_english(dataset_dict):\n",
        "    filtered_dict = {}\n",
        "    for phase, dataset in dataset_dict.items():\n",
        "        print(f\"\\nFiltering {phase}...\")\n",
        "        filter_fn, counter = make_filter_fn()\n",
        "        filtered_dataset = dataset.filter(\n",
        "            filter_fn,\n",
        "            desc=f\"Filtering {phase}\",\n",
        "            num_proc=1,\n",
        "            with_indices=False\n",
        "        )\n",
        "        total = len(dataset)\n",
        "        kept = counter[\"kept_en\"] + counter[\"kept_system\"]\n",
        "        removed = counter[\"removed\"]\n",
        "        print(f\"{phase} Summary:\")\n",
        "        print(f\"  Total entries:        {total:,}\")\n",
        "        print(f\"  Kept (English):       {counter['kept_en']:,}\")\n",
        "        print(f\"  Kept (system):        {counter['kept_system']:,}\")\n",
        "        print(f\"  Removed (non-English): {removed:,} ({(removed/total)*100:.2f}%)\")\n",
        "        filtered_dict[phase] = filtered_dataset\n",
        "    return DatasetDict(filtered_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63MPg4T_zPbl"
      },
      "outputs": [],
      "source": [
        "dataset_fasttext = filter_english(dataset_injected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z76_4hSxU6I_"
      },
      "outputs": [],
      "source": [
        "dataset_injected = dataset_fasttext.remove_columns('__index_level_0__')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRNy1TTo9NU4"
      },
      "outputs": [],
      "source": [
        "dataset_injected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg0eU0ov60MJ"
      },
      "outputs": [],
      "source": [
        "print(calculate_avg_length(dataset_injected))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xazvGmyM6VC_"
      },
      "outputs": [],
      "source": [
        "def split_dataset_dict(dataset_dict):\n",
        "\n",
        "    dataset_injected_first = {}\n",
        "    dataset_injected_second = {}\n",
        "    dataset_injected_third = {}\n",
        "    dataset_injected_fourth = {}\n",
        "\n",
        "    for phase_name, dataset in dataset_dict.items():\n",
        "        # Calculate split points (divide into 4 equal parts)\n",
        "        total_rows = len(dataset)\n",
        "        quarter = total_rows // 4\n",
        "\n",
        "        # Calculate split indices\n",
        "        split_1 = quarter\n",
        "        split_2 = quarter * 2\n",
        "        split_3 = quarter * 3\n",
        "\n",
        "        # Split the dataset into 4 parts\n",
        "        first_quarter = dataset.select(range(0, split_1))\n",
        "        second_quarter = dataset.select(range(split_1, split_2))\n",
        "        third_quarter = dataset.select(range(split_2, split_3))\n",
        "        fourth_quarter = dataset.select(range(split_3, total_rows))\n",
        "\n",
        "        # Add to respective dictionaries\n",
        "        dataset_injected_first[phase_name] = first_quarter\n",
        "        dataset_injected_second[phase_name] = second_quarter\n",
        "        dataset_injected_third[phase_name] = third_quarter\n",
        "        dataset_injected_fourth[phase_name] = fourth_quarter\n",
        "\n",
        "        print(f\"{phase_name}: Split {total_rows} rows into {len(first_quarter)} + {len(second_quarter)} + {len(third_quarter)} + {len(fourth_quarter)} rows\")\n",
        "\n",
        "    # Convert dictionaries back to DatasetDict objects\n",
        "    dataset_injected_first = DatasetDict(dataset_injected_first)\n",
        "    dataset_injected_second = DatasetDict(dataset_injected_second)\n",
        "    dataset_injected_third = DatasetDict(dataset_injected_third)\n",
        "    dataset_injected_fourth = DatasetDict(dataset_injected_fourth)\n",
        "\n",
        "    return dataset_injected_first, dataset_injected_second, dataset_injected_third, dataset_injected_fourth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhOnYzct6Wsa"
      },
      "outputs": [],
      "source": [
        "dataset_injected_first, dataset_injected_second, dataset_injected_third, dataset_injected_fourth = split_dataset_dict(dataset_injected)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Categorized Dataset"
      ],
      "metadata": {
        "id": "GBCnu9w2ERff"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahBoqX-Uk_Ry"
      },
      "source": [
        "## Create JSONL File for OpenAI Batch API"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this section creates a batch processing system for classifying prompt injection attacks using OpenAI's API. It defines a function `create_jsonl_file()` that formats individual text samples into JSON requests for GPT-3.5-turbo, where each request asks the model to classify prompts into one of five cybersecurity attack categories: jailbreak, system leak, task override, encoding manipulation, or prompt confusion. The system uses a detailed prompt that instructs the AI to act as a cybersecurity expert and classify injection attempts based on specific definitions for each attack type."
      ],
      "metadata": {
        "id": "kKDl8NDr_fcG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PnqAsWlyiHX"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import time\n",
        "import json\n",
        "counter = 0\n",
        "def create_jsonl_file(batch, field, output_file=\"batch_requests.jsonl\", model=\"gpt-3.5-turbo-0125\"):\n",
        "    global counter\n",
        "    text = batch[field]\n",
        "    if not text:\n",
        "        return \"Error with Texts\"\n",
        "\n",
        "    try:\n",
        "        counter += 1\n",
        "        custom_id = f\"batch-{counter}\"\n",
        "\n",
        "        request_data = {\n",
        "            \"custom_id\": custom_id,\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": model,\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"\"\"You are a cybersecurity expert specializing in prompt injection classification.\n",
        "\n",
        "                                      You will receive injected prompts and must classify them into one of these attack categories:\n",
        "\n",
        "                                      1. **jailbreak** — Only prompts designed to bypass the model's safety restrictions, override ethical constraints, or make the AI ignore its built-in instructions. Often involves pretending to be someone else, using reverse psychology, or roleplay.\n",
        "                                      2. **system leak** — Only prompts aimed at exposing hidden system messages, configurations, internal logic, or developer instructions that are not meant for the user.\n",
        "                                      3. **task override** — Only prompts that attempt to change or override the original objective assigned to the model. The attacker tries to reprogram the task, shift the topic, or redefine the goal.\n",
        "                                      4. **encoding manipulation** — Only prompts that hide malicious instructions using THESE ENCODED FORMATS (base64, hex, Unicode, URL encoding) to bypass filters or detection systems.\n",
        "                                      5. **prompt confusion** — Only prompts that use misleading, contradictory, repetitive wording, or ambiguous instructions to confuse the model's behavior. These attacks often involve multiple, conflicting tasks or embedded triggers.\n",
        "\n",
        "                                      Respond with only one category name (e.g., \"jailbreak\", \"system_leak\", \"task_override\", \"encoding_manipulation\", \"prompt_confusion\") with no markdown or any symbols before the category name.\"\"\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"You will receive a prompt. For that prompt, respond with the category (one of: jailbreak, system_leak, task_override, encoding_manipulation, prompt_confusion) in that format. Take into account the exact definitions for each type of injection and do not return that the prompt isn't injected. Here is the prompt:\\n\\n {text}\"\n",
        "                    }\n",
        "                ],\n",
        "                \"max_tokens\": 20,\n",
        "                \"temperature\": 0.1\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(output_file, 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(request_data) + '\\n')\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating batch file: {e}\")\n",
        "\n",
        "def allocate_dataset(dataset_injected, field, batch_size=1, output_file=\"batch_requests.jsonl\"):\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for split_name, dataset in dataset_injected.items():\n",
        "        print(f\"Processing {split_name} split with {len(dataset)} samples...\")\n",
        "\n",
        "        dataset.map (\n",
        "            lambda batch: create_jsonl_file(batch, field, output_file),\n",
        "            batched=True,\n",
        "            batch_size=batch_size,\n",
        "            desc=f\"Creating batch requests for {split_name}\"\n",
        "        )\n",
        "\n",
        "    print(f\"All batch requests written to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul5gCJZCtl4Z"
      },
      "source": [
        "## Use the OpenAI Batch API to allocate categories"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this section processes large datasets for prompt injection classification by splitting them into chunks and submitting each chunk as a separate batch job to OpenAI's API. The workflow follows a consistent pattern: generate JSONL request files, upload them to OpenAI, create batch jobs with 24-hour completion windows, and retrieve the classification results. By dividing the work into multiple batches (first, second1, second2, third, fourth), it efficiently handles large-scale data processing while staying within API limits. Each batch is tracked with descriptive metadata to manage the multiple concurrent operations systematically."
      ],
      "metadata": {
        "id": "3YRS-b5l_tHL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ct5ag9xsWbE"
      },
      "outputs": [],
      "source": [
        "allocate_dataset(dataset_injected_first, \"body\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_6yFbdftyIR"
      },
      "outputs": [],
      "source": [
        "batch_input_file = client.files.create(\n",
        "    file=open(\"batch_requests.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "\n",
        "print(batch_input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfMKPcZkv2S9"
      },
      "outputs": [],
      "source": [
        "batch_input_file_id = batch_input_file.id\n",
        "batch_val = client.batches.create(\n",
        "    input_file_id=batch_input_file_id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"first quarter\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuH-cJO_v3La"
      },
      "outputs": [],
      "source": [
        "batch = client.batches.retrieve(batch_val.id)\n",
        "print(batch)\n",
        "batch_output_file_id = batch.output_file_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xltv2sZWjON-"
      },
      "outputs": [],
      "source": [
        "file_response = client.files.content(batch_output_file_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "px1ZMReT7QKw"
      },
      "outputs": [],
      "source": [
        "print(file_response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVZnHuhbf80g"
      },
      "outputs": [],
      "source": [
        "# Get the current batch status\n",
        "batch = client.batches.retrieve(batch.id)\n",
        "print(f\"Batch status: {batch.status}\")\n",
        "\n",
        "# Only try to access error file if batch failed or completed with errors\n",
        "if batch.status in [\"failed\", \"completed\"] and batch.error_file_id:\n",
        "    error_file_response = client.files.content(batch.error_file_id)\n",
        "    print(error_file_response.text)\n",
        "elif batch.status == \"completed\":\n",
        "    print(\"Batch completed successfully - no errors to display\")\n",
        "else:\n",
        "    print(f\"Batch is {batch.status} - error file not yet available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHVR3No_I_P9"
      },
      "outputs": [],
      "source": [
        "def split_dataset_dict_half(dataset_dict):\n",
        "\n",
        "    dataset_injected_first = {}\n",
        "    dataset_injected_second = {}\n",
        "\n",
        "    for phase_name, dataset in dataset_dict.items():\n",
        "        # Calculate split point (divide into 2 equal parts)\n",
        "        total_rows = len(dataset)\n",
        "        half = total_rows // 2\n",
        "\n",
        "        # Split the dataset into 2 parts\n",
        "        first_half = dataset.select(range(0, half))\n",
        "        second_half = dataset.select(range(half, total_rows))\n",
        "\n",
        "        # Add to respective dictionaries\n",
        "        dataset_injected_first[phase_name] = first_half\n",
        "        dataset_injected_second[phase_name] = second_half\n",
        "\n",
        "        print(f\"{phase_name}: Split {total_rows} rows into {len(first_half)} + {len(second_half)} rows\")\n",
        "\n",
        "    # Convert dictionaries back to DatasetDict objects\n",
        "    dataset_injected_first = DatasetDict(dataset_injected_first)\n",
        "    dataset_injected_second = DatasetDict(dataset_injected_second)\n",
        "\n",
        "    return dataset_injected_first, dataset_injected_second"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD2f9il18ce4"
      },
      "outputs": [],
      "source": [
        "dataset_injected_second1, dataset_injected_second2 = split_dataset_dict_half(dataset_injected_second)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw5cN7xx8nIv"
      },
      "outputs": [],
      "source": [
        "allocate_dataset(dataset_injected_second1, \"body\", output_file=\"batch_requests_second1.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICLutgQ081jY"
      },
      "outputs": [],
      "source": [
        "batch_input_file_second1 = client.files.create(\n",
        "    file=open(\"batch_requests_second1.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "\n",
        "print(batch_input_file_second1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqqI4Dsy9Bmi"
      },
      "outputs": [],
      "source": [
        "batch_input_file_id_second1 = batch_input_file_second1.id\n",
        "batch_val_second1 = client.batches.create(\n",
        "    input_file_id=batch_input_file_id_second1,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"second part 1\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtjOaK_39F3d"
      },
      "outputs": [],
      "source": [
        "batch_second1 = client.batches.retrieve(batch_val_second1.id)\n",
        "print(batch_second1)\n",
        "batch_output_file_id_second1 = batch_second1.output_file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHA5imdc3kz6"
      },
      "outputs": [],
      "source": [
        "file_response_second1 = client.files.content(batch_output_file_id_second1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82VhosiZZ3sX"
      },
      "outputs": [],
      "source": [
        "file_response_second1.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRSDK9qLJ6NJ"
      },
      "outputs": [],
      "source": [
        "allocate_dataset(dataset_injected_second2, \"body\", output_file=\"batch_requests_second2.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5S6zk_IJ-Qk"
      },
      "outputs": [],
      "source": [
        "batch_input_file_second2 = client.files.create(\n",
        "    file=open(\"batch_requests_second2.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "\n",
        "print(batch_input_file_second2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4miUmK1eKDgX"
      },
      "outputs": [],
      "source": [
        "batch_input_file_id_second2 = batch_input_file_second2.id\n",
        "batch_val_second2 = client.batches.create(\n",
        "    input_file_id=batch_input_file_id_second2,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"second part 2\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xlM4lf1KD6_"
      },
      "outputs": [],
      "source": [
        "batch_second2 = client.batches.retrieve(batch_val_second2.id)\n",
        "print(batch_second2)\n",
        "batch_output_file_id_second2 = batch_second2.output_file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9m4noJVKmDH"
      },
      "outputs": [],
      "source": [
        "file_response_second2 = client.files.content(batch_output_file_id_second2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oSMZdXZZCfb"
      },
      "outputs": [],
      "source": [
        "file_response_second2.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSm0mG1lL1mr"
      },
      "outputs": [],
      "source": [
        "allocate_dataset(dataset_injected_third, \"body\", output_file=\"batch_requests_third.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wIaErx5MK1Q"
      },
      "outputs": [],
      "source": [
        "batch_input_file_third = client.files.create(\n",
        "    file=open(\"batch_requests_third.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "\n",
        "print(batch_input_file_third)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTo9VtuUMQu0"
      },
      "outputs": [],
      "source": [
        "batch_input_file_id_third = batch_input_file_third.id\n",
        "batch_val_third = client.batches.create(\n",
        "    input_file_id=batch_input_file_id_third,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"third part\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbXn4YmBMWfR"
      },
      "outputs": [],
      "source": [
        "batch_third = client.batches.retrieve(batch_val_third.id)\n",
        "print(batch_third)\n",
        "batch_output_file_id_third = batch_third.output_file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Osm5ul33xKN"
      },
      "outputs": [],
      "source": [
        "file_response_third = client.files.content(batch_output_file_id_third)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xutwg8E8Oacm"
      },
      "outputs": [],
      "source": [
        "allocate_dataset(dataset_injected_fourth, \"body\", output_file=\"batch_requests_fourth.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVbA9NLKOoyf"
      },
      "outputs": [],
      "source": [
        "batch_input_file_fourth = client.files.create(\n",
        "    file=open(\"batch_requests_fourth.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "\n",
        "print(batch_input_file_fourth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-e4GgXyOvEG"
      },
      "outputs": [],
      "source": [
        "batch_input_file_id_fourth = batch_input_file_fourth.id\n",
        "batch_val_fourth = client.batches.create(\n",
        "    input_file_id=batch_input_file_id_fourth,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"fourth part\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sll5CgPcO2jC"
      },
      "outputs": [],
      "source": [
        "batch_fourth = client.batches.retrieve(batch_val_fourth.id)\n",
        "print(batch_fourth)\n",
        "batch_output_file_id_fourth = batch_fourth.output_file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLDCgKMY5TKE"
      },
      "outputs": [],
      "source": [
        "file_response_fourth = client.files.content(batch_output_file_id_fourth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NqtxjNq4UER"
      },
      "outputs": [],
      "source": [
        "print(file_response_fourth.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5E72zhjJY50"
      },
      "source": [
        "## Process Batch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this section processes OpenAI batch API responses to add prompt injection classification categories to datasets. The `process_batch_and_add_categories` function parses batch responses, extracts categories (jailbreak, system leak, etc.), and adds them as new columns to the original datasets. It handles multiple dataset chunks, concatenates them back into Phase1/Phase2 splits, and includes error handling for failed classifications. The processed labeled dataset is then saved to Google Drive as JSON and can be reloaded as a DatasetDict for further analysis."
      ],
      "metadata": {
        "id": "z1s4igS3_3eu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDmqxalQJZvc"
      },
      "outputs": [],
      "source": [
        "def process_batch_and_add_categories(original_dataset, batch_content, batch_size=1, filter_failed=True):\n",
        "    \"\"\"\n",
        "    Process batch responses and add categories to dataset\n",
        "\n",
        "    Args:\n",
        "        original_dataset: HuggingFace DatasetDict\n",
        "        batch_content: String containing batch responses (JSON/JSONL format)\n",
        "        batch_size: Size of each batch (default=1)\n",
        "        filter_failed: Whether to filter out failed entries (default=True)\n",
        "    \"\"\"\n",
        "    from datasets import DatasetDict, Dataset\n",
        "\n",
        "    print(\"Starting batch processing...\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Filter failed: {filter_failed}\")\n",
        "\n",
        "\n",
        "    # Step 1: Parse the batch content\n",
        "    print(\"Parsing batch responses...\")\n",
        "    batch_responses = {}\n",
        "\n",
        "    # Parse the batch content\n",
        "    batch_data = parse_batch_content(batch_content)\n",
        "\n",
        "    if not batch_data:\n",
        "        print(\"ERROR: No batch data could be parsed from the input!\")\n",
        "        print(f\"Input content preview: {repr(batch_content[:500] if batch_content else 'None')}...\")\n",
        "        return original_dataset\n",
        "\n",
        "    print(f\"Successfully parsed {len(batch_data)} batch responses\")\n",
        "\n",
        "    # Process each response\n",
        "    for response_data in batch_data:\n",
        "        try:\n",
        "            custom_id = response_data['custom_id']\n",
        "\n",
        "            # Extract content from response\n",
        "            content = response_data['response']['body']['choices'][0]['message']['content']\n",
        "\n",
        "            # Parse categories from content\n",
        "            categories = []\n",
        "\n",
        "            # For batch_size=1, we expect a single category\n",
        "            if batch_size == 1:\n",
        "                category = extract_category_from_text(content)\n",
        "                if category and is_valid_category(category):\n",
        "                    categories = [category]\n",
        "                else:\n",
        "                    print(f\"Invalid category extracted from {custom_id}: '{category}' from content: '{content}'\")\n",
        "                    categories = ['failed_parsing']\n",
        "            else:\n",
        "                # Handle multiple categories (your existing logic)\n",
        "                content_lines = content.split('\\n')\n",
        "\n",
        "                for line_content in content_lines:\n",
        "                    line_content = line_content.strip()\n",
        "                    if not line_content:\n",
        "                        continue\n",
        "\n",
        "                    # Handle multiple categories in one line\n",
        "                    if ',' in line_content or '=' in line_content:\n",
        "                        potential_parts = []\n",
        "                        for sep in [',', '=', ';', '|']:\n",
        "                            if sep in line_content:\n",
        "                                potential_parts = line_content.split(sep)\n",
        "                                break\n",
        "\n",
        "                        if potential_parts:\n",
        "                            for part in potential_parts:\n",
        "                                part = part.strip()\n",
        "                                if part:\n",
        "                                    category = extract_category_from_text(part)\n",
        "                                    if category and is_valid_category(category):\n",
        "                                        categories.append(category)\n",
        "                        continue\n",
        "\n",
        "                    # Single category per line\n",
        "                    category = extract_category_from_text(line_content)\n",
        "                    if category and is_valid_category(category):\n",
        "                        categories.append(category)\n",
        "\n",
        "                # Validate we have the expected number of categories\n",
        "                if len(categories) != batch_size:\n",
        "                    print(f\"Expected {batch_size} categories for {custom_id}, but found {len(categories)}\")\n",
        "                    print(f\"Categories found: {categories}\")\n",
        "                    print(f\"Raw content: {repr(content)}\")\n",
        "\n",
        "                    # Adjust categories list\n",
        "                    if len(categories) < batch_size:\n",
        "                        missing = batch_size - len(categories)\n",
        "                        categories.extend(['failed_parsing'] * missing)\n",
        "                    else:\n",
        "                        categories = categories[:batch_size]\n",
        "\n",
        "            batch_responses[custom_id] = categories\n",
        "            print(f\"Batch {custom_id}: Found {len(categories)} categories: {categories}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing batch response: {e}\")\n",
        "            print(f\"Response data keys: {list(response_data.keys()) if isinstance(response_data, dict) else 'Not a dict'}\")\n",
        "            if isinstance(response_data, dict):\n",
        "                print(f\"Custom ID: {response_data.get('custom_id', 'MISSING')}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Found {len(batch_responses)} successful batches\")\n",
        "\n",
        "    # Step 2: Process each split\n",
        "    updated_splits = {}\n",
        "    failed_batches = set()\n",
        "\n",
        "    # First, let's see what custom_ids we actually have\n",
        "    available_custom_ids = sorted(batch_responses.keys())\n",
        "    print(f\"Available custom_ids: {available_custom_ids[:10]}...\" if len(available_custom_ids) > 10 else f\"Available custom_ids: {available_custom_ids}\")\n",
        "\n",
        "    # Extract just the batch numbers to understand the sequence\n",
        "    batch_numbers = []\n",
        "    for custom_id in available_custom_ids:\n",
        "        try:\n",
        "            parts = custom_id.split('-')\n",
        "            if len(parts) >= 2:\n",
        "                batch_num = int(parts[1])\n",
        "                batch_numbers.append(batch_num)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    if batch_numbers:\n",
        "        print(f\"Batch number range: {min(batch_numbers)} to {max(batch_numbers)} ({len(batch_numbers)} total)\")\n",
        "\n",
        "    for split_name, dataset in original_dataset.items():\n",
        "        print(f\"\\nProcessing {split_name}...\")\n",
        "\n",
        "        split_categories = []\n",
        "        num_samples = len(dataset)\n",
        "\n",
        "        print(f\"Dataset has {num_samples} samples\")\n",
        "\n",
        "        # Create a mapping based on available custom_ids\n",
        "        # We'll match them in order to the dataset samples\n",
        "        sorted_custom_ids = sorted(batch_responses.keys(), key=lambda x: int(x.split('-')[1]) if len(x.split('-')) > 1 and x.split('-')[1].isdigit() else 0)\n",
        "\n",
        "        samples_processed = 0\n",
        "        custom_id_index = 0\n",
        "\n",
        "        # Go through samples and match with available batch responses\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            current_batch_size = min(batch_size, num_samples - i)\n",
        "\n",
        "            if custom_id_index < len(sorted_custom_ids):\n",
        "                # Use the next available custom_id\n",
        "                custom_id = sorted_custom_ids[custom_id_index]\n",
        "                custom_id_index += 1\n",
        "\n",
        "                if custom_id in batch_responses:\n",
        "                    batch_cats = batch_responses[custom_id]\n",
        "\n",
        "                    # Ensure we have the right number of categories\n",
        "                    if len(batch_cats) == current_batch_size:\n",
        "                        split_categories.extend(batch_cats)\n",
        "                        print(f\"✓ {custom_id}: Added {len(batch_cats)} categories\")\n",
        "                    else:\n",
        "                        print(f\"⚠ {custom_id}: Expected {current_batch_size} categories, got {len(batch_cats)}\")\n",
        "                        # Take what we have and fill the rest\n",
        "                        split_categories.extend(batch_cats[:current_batch_size])\n",
        "                        if len(batch_cats) < current_batch_size:\n",
        "                            missing = current_batch_size - len(batch_cats)\n",
        "                            split_categories.extend(['failed_parsing'] * missing)\n",
        "                else:\n",
        "                    print(f\"✗ {custom_id}: Not found in responses\")\n",
        "                    split_categories.extend(['failed'] * current_batch_size)\n",
        "                    failed_batches.add(custom_id)\n",
        "            else:\n",
        "                # No more custom_ids available\n",
        "                print(f\"✗ No more batch responses available - adding {current_batch_size} 'failed' entries\")\n",
        "                split_categories.extend(['failed'] * current_batch_size)\n",
        "\n",
        "            samples_processed += current_batch_size\n",
        "\n",
        "        # Validation\n",
        "        print(f\"Generated {len(split_categories)} categories for {num_samples} samples\")\n",
        "\n",
        "        if len(split_categories) != num_samples:\n",
        "            print(f\"ERROR: Category count mismatch!\")\n",
        "            print(f\"Expected: {num_samples}, Got: {len(split_categories)}\")\n",
        "\n",
        "            if len(split_categories) < num_samples:\n",
        "                missing = num_samples - len(split_categories)\n",
        "                print(f\"Adding {missing} 'missing' entries\")\n",
        "                split_categories.extend(['missing'] * missing)\n",
        "            elif len(split_categories) > num_samples:\n",
        "                print(f\"Truncating to {num_samples} entries\")\n",
        "                split_categories = split_categories[:num_samples]\n",
        "\n",
        "        # Count categories\n",
        "        category_counts = {}\n",
        "        for cat in split_categories:\n",
        "            category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "        print(f\"Category distribution for {split_name}:\")\n",
        "        for cat, count in sorted(category_counts.items()):\n",
        "            print(f\"  {cat}: {count}\")\n",
        "\n",
        "        # Add category column\n",
        "        try:\n",
        "            dataset_with_categories = dataset.add_column('category', split_categories)\n",
        "            updated_splits[split_name] = dataset_with_categories\n",
        "            print(f\"✓ Successfully added categories to {split_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error adding categories to {split_name}: {e}\")\n",
        "            raise\n",
        "\n",
        "    # Create DatasetDict\n",
        "    dataset_dict = DatasetDict(updated_splits)\n",
        "\n",
        "    # Step 3: Apply filtering if requested\n",
        "    if filter_failed:\n",
        "        print(\"\\nApplying filtering to remove failed entries...\")\n",
        "        dataset_dict = filter_failed_parsing_datasetdict(dataset_dict)\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\nSUMMARY:\")\n",
        "    for split_name, split_dataset in dataset_dict.items():\n",
        "        print(f\"{split_name}: {len(split_dataset)} samples\")\n",
        "\n",
        "    if failed_batches:\n",
        "        print(f\"Failed batches: {sorted(failed_batches)}\")\n",
        "\n",
        "    return dataset_dict\n",
        "\n",
        "\n",
        "def parse_batch_content(batch_content):\n",
        "    \"\"\"\n",
        "    Robust parser for batch content (JSON/JSONL format)\n",
        "    \"\"\"\n",
        "    import json\n",
        "\n",
        "    if not batch_content:\n",
        "        print(\"Empty batch content received\")\n",
        "        return []\n",
        "\n",
        "    # Handle different input types\n",
        "    if hasattr(batch_content, 'text'):\n",
        "        batch_content = batch_content.text\n",
        "    elif not isinstance(batch_content, str):\n",
        "        batch_content = str(batch_content)\n",
        "\n",
        "    batch_content = batch_content.strip()\n",
        "\n",
        "    if not batch_content:\n",
        "        print(\"Empty batch content after processing\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Content length: {len(batch_content)} characters\")\n",
        "    print(f\"Content starts with: {repr(batch_content[:100])}\")\n",
        "    print(f\"Content ends with: {repr(batch_content[-100:])}\")\n",
        "\n",
        "    responses = []\n",
        "\n",
        "    try:\n",
        "        # Method 1: Try JSONL format (most common)\n",
        "        print(\"Attempting JSONL parsing...\")\n",
        "        lines = batch_content.split('\\n')\n",
        "        print(f\"Found {len(lines)} lines\")\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                response = json.loads(line)\n",
        "\n",
        "                # Validate response structure\n",
        "                if 'custom_id' not in response:\n",
        "                    print(f\"Line {i+1}: Missing custom_id\")\n",
        "                    continue\n",
        "\n",
        "                if 'response' not in response:\n",
        "                    print(f\"Line {i+1}: Missing response field\")\n",
        "                    continue\n",
        "\n",
        "                # Check for error field\n",
        "                if response.get('error'):\n",
        "                    print(f\"Line {i+1}: Response has error: {response['error']}\")\n",
        "                    continue\n",
        "\n",
        "                # Validate nested structure\n",
        "                try:\n",
        "                    content = response['response']['body']['choices'][0]['message']['content']\n",
        "                    responses.append(response)\n",
        "\n",
        "                    if i < 5:  # Show first few for debugging\n",
        "                        print(f\"✓ Line {i+1}: {response['custom_id']} -> '{content}'\")\n",
        "\n",
        "                except (KeyError, IndexError, TypeError) as e:\n",
        "                    print(f\"Line {i+1}: Invalid response structure: {e}\")\n",
        "                    continue\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Line {i+1}: JSON decode error: {e}\")\n",
        "                if len(line) < 200:\n",
        "                    print(f\"  Full line: {repr(line)}\")\n",
        "                else:\n",
        "                    print(f\"  Line preview: {repr(line[:100])}...{repr(line[-100:])}\")\n",
        "                continue\n",
        "\n",
        "        if responses:\n",
        "            print(f\"Successfully parsed {len(responses)} responses from JSONL\")\n",
        "            return responses\n",
        "\n",
        "        # Method 2: Try JSON array format\n",
        "        print(\"JSONL failed, attempting JSON array parsing...\")\n",
        "        if batch_content.startswith('[') and batch_content.endswith(']'):\n",
        "            data = json.loads(batch_content)\n",
        "            if isinstance(data, list):\n",
        "                print(f\"Successfully parsed {len(data)} responses from JSON array\")\n",
        "                return data\n",
        "\n",
        "        # Method 3: Try single JSON object\n",
        "        print(\"Attempting single JSON object parsing...\")\n",
        "        data = json.loads(batch_content)\n",
        "        if isinstance(data, dict):\n",
        "            if 'responses' in data:\n",
        "                return data['responses']\n",
        "            elif 'data' in data:\n",
        "                return data['data']\n",
        "            else:\n",
        "                return [data]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"All parsing methods failed: {e}\")\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "def extract_category_from_text(text):\n",
        "    \"\"\"\n",
        "    Extract category from text, handling various formats\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if not text or not isinstance(text, str):\n",
        "        return None\n",
        "\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    # Handle numbered format: \"1. category\" or \"2. system_leak\"\n",
        "    numbered_match = re.match(r'^\\d+\\.\\s*(.+)', text)\n",
        "    if numbered_match:\n",
        "        category = numbered_match.group(1).strip()\n",
        "    # Handle bullet formats: \"- category\" or \"* category\"\n",
        "    elif text.startswith(('- ', '* ')):\n",
        "        category = text[2:].strip()\n",
        "    # Handle colon format: \"Category: value\"\n",
        "    elif ':' in text:\n",
        "        category = text.split(':', 1)[1].strip()\n",
        "    else:\n",
        "        category = text\n",
        "\n",
        "    # Clean formatting\n",
        "    category = re.sub(r'[*`\"\\'()[\\]{}]', '', category).strip()\n",
        "\n",
        "    # Remove common prefixes\n",
        "    prefixes = ['category', 'type', 'classification', 'label', 'answer', 'result']\n",
        "    category_lower = category.lower()\n",
        "    for prefix in prefixes:\n",
        "        if category_lower.startswith(prefix + ':'):\n",
        "            category = category[len(prefix)+1:].strip()\n",
        "            break\n",
        "        elif category_lower.startswith(prefix + ' '):\n",
        "            category = category[len(prefix)+1:].strip()\n",
        "            break\n",
        "\n",
        "    # Convert to lowercase\n",
        "    category = category.lower().strip()\n",
        "\n",
        "    return category if category else None\n",
        "\n",
        "\n",
        "def is_valid_category(category):\n",
        "    \"\"\"\n",
        "    Check if category is valid\n",
        "    \"\"\"\n",
        "    if not category or len(category) < 3:\n",
        "        return False\n",
        "\n",
        "    valid_categories = {\n",
        "        'jailbreak',\n",
        "        'system_leak',\n",
        "        'task_override',\n",
        "        'encoding_manipulation',\n",
        "        'prompt_confusion'\n",
        "    }\n",
        "\n",
        "    return category.lower() in valid_categories\n",
        "\n",
        "\n",
        "def filter_failed_parsing_datasetdict(dataset_dict):\n",
        "    \"\"\"\n",
        "    Filter out failed entries from DatasetDict\n",
        "    \"\"\"\n",
        "    from datasets import DatasetDict\n",
        "\n",
        "    filtered_dict = {}\n",
        "    failure_types = {'failed', 'failed_parsing', 'missing'}\n",
        "\n",
        "    for phase_name, dataset in dataset_dict.items():\n",
        "        print(f\"\\nFiltering {phase_name}...\")\n",
        "\n",
        "        # Show before filtering\n",
        "        before_count = len(dataset)\n",
        "        category_counts = {}\n",
        "        for example in dataset:\n",
        "            cat = example['category']\n",
        "            category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "        print(f\"Before filtering ({before_count} samples):\")\n",
        "        for cat, count in sorted(category_counts.items()):\n",
        "            print(f\"  {cat}: {count}\")\n",
        "\n",
        "        # Filter out failure types\n",
        "        filtered_dataset = dataset.filter(lambda example: example['category'] not in failure_types)\n",
        "        filtered_dict[phase_name] = filtered_dataset\n",
        "\n",
        "        # Show after filtering\n",
        "        after_count = len(filtered_dataset)\n",
        "        print(f\"After filtering: {before_count} -> {after_count} ({before_count - after_count} removed)\")\n",
        "\n",
        "    return DatasetDict(filtered_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRPB2zufhwRc"
      },
      "outputs": [],
      "source": [
        "updated_dataset_part_one = process_batch_and_add_categories(\n",
        "     original_dataset=dataset_injected_first,\n",
        "     batch_content=file_response.text,\n",
        "     batch_size=1\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n8OQcUp3ZCC"
      },
      "outputs": [],
      "source": [
        "updated_dataset_part_two1 = process_batch_and_add_categories(\n",
        "     original_dataset=dataset_injected_second1,\n",
        "     batch_content=file_response_second1.text,\n",
        "     batch_size=1\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWSev42vtb8N"
      },
      "outputs": [],
      "source": [
        "updated_dataset_part_two2 = process_batch_and_add_categories(\n",
        "     original_dataset=dataset_injected_second2,\n",
        "     batch_content=file_response_second2.text,\n",
        "     batch_size=1\n",
        " )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KErvlX2mtl65"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "phase1 = concatenate_datasets([updated_dataset_part_two1[\"Phase1\"], updated_dataset_part_two2[\"Phase1\"]])\n",
        "phase2 = concatenate_datasets([updated_dataset_part_two1[\"Phase2\"], updated_dataset_part_two2[\"Phase2\"]])\n",
        "\n",
        "updated_dataset_part_two = DatasetDict({\"Phase1\": phase1, \"Phase2\":phase2})\n",
        "updated_dataset_part_two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKorR61g6VkB"
      },
      "outputs": [],
      "source": [
        "updated_dataset_part_three = process_batch_and_add_categories(\n",
        "     original_dataset=dataset_injected_third,\n",
        "     batch_content=file_response_third.text,\n",
        "     batch_size=1\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_33_XMd6bJm"
      },
      "outputs": [],
      "source": [
        "updated_dataset_part_four = process_batch_and_add_categories(\n",
        "     original_dataset=dataset_injected_fourth,\n",
        "     batch_content=file_response_fourth.text,\n",
        "     batch_size=1\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlqsWxRuAL5O"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "updated_dataset_full_phase1 = concatenate_datasets([updated_dataset_part_one[\"Phase1\"], updated_dataset_part_two[\"Phase1\"], updated_dataset_part_three[\"Phase1\"], updated_dataset_part_four[\"Phase1\"]])\n",
        "updated_dataset_full_phase2 = concatenate_datasets([updated_dataset_part_one[\"Phase2\"], updated_dataset_part_two[\"Phase2\"], updated_dataset_part_three[\"Phase2\"], updated_dataset_part_four[\"Phase2\"]])\n",
        "\n",
        "updated_dataset_full = DatasetDict({\"Phase1\":updated_dataset_full_phase1, \"Phase2\":updated_dataset_full_phase2})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJpxikBqvA35"
      },
      "outputs": [],
      "source": [
        "updated_dataset_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxF9Za7CspAr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlxQUNDmDP0X"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Algoverse/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "json_data = {}\n",
        "\n",
        "for phase_name, dataset in updated_dataset_full.items():\n",
        "    print(f\"Processing {phase_name}...\")\n",
        "\n",
        "\n",
        "    phase_data = []\n",
        "    for i in range(len(dataset)):\n",
        "        row = {}\n",
        "        for feature in dataset.features:\n",
        "            row[feature] = dataset[i][feature]\n",
        "        phase_data.append(row)\n",
        "\n",
        "    json_data[phase_name] = {\n",
        "        'features': list(dataset.features.keys()),\n",
        "        'num_rows': len(dataset),\n",
        "        'data': phase_data\n",
        "    }\n",
        "\n",
        "\n",
        "output_path = os.path.join(output_dir, 'dataset_with_categories.json')\n",
        "\n",
        "print(f\"Saving data to {output_path}...\")\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "print(f\"Successfully saved dataset to {output_path}\")\n",
        "print(f\"File size: {os.path.getsize(output_path)} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ex42DYYxhGs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset, DatasetDict\n",
        "json_file_path = '/content/drive/MyDrive/Algoverse/dataset_with_categories.json'\n",
        "\n",
        "print(f\"Loading data from {json_file_path}...\")\n",
        "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "dataset_dict = {}\n",
        "for phase_name, phase_info in json_data.items():\n",
        "    print(f\"Processing {phase_name}...\")\n",
        "\n",
        "    dataset_dict[phase_name] = Dataset.from_list(phase_info['data'])\n",
        "\n",
        "\n",
        "updated_dataset_full = DatasetDict(dataset_dict)\n",
        "\n",
        "print(\"Successfully loaded DatasetDict!\")\n",
        "\n",
        "updated_dataset_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuwWOTMUDRf6"
      },
      "outputs": [],
      "source": [
        "print(calculate_avg_length(updated_dataset_full))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add pairs for the injected prompts"
      ],
      "metadata": {
        "id": "oF4i8tmaEfHT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0SUz-say63M"
      },
      "source": [
        "## Create JSONL File for Cleaning (OpenAI BatchAPI)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this section defines a system for generating cleaned versions of prompt injection attacks by sending the original malicious prompts along with their classifications to OpenAI’s API to remove only the injected portions. The `create_json_file_clean` function creates batch requests that ask GPT to reconstruct safe versions by removing injection parts without adding new content, while the `process_batch_and_add_pairs` function processes the API responses and adds the cleaned text as a new “pair” column to create training pairs of malicious and cleaned prompts.\n"
      ],
      "metadata": {
        "id": "RXrJf22DAPO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2m-0vj_zNB3"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import tiktoken\n",
        "counter = 0\n",
        "def create_jsonl_file_clean(batch, field, output_file=\"batch_requests_clean.jsonl\", model=\"gpt-3.5-turbo-0125\"):\n",
        "    global counter\n",
        "    text = batch[field]\n",
        "    category = batch[\"category\"]\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0125\")\n",
        "    text = text[0]\n",
        "    tokens = encoding.encode(text, disallowed_special=())\n",
        "    num_tokens = len(tokens)\n",
        "    if not text:\n",
        "        return \"Error with Texts\"\n",
        "\n",
        "    try:\n",
        "        counter += 1\n",
        "        custom_id = f\"batch-{counter}\"\n",
        "\n",
        "        request_data = {\n",
        "            \"custom_id\": custom_id,\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "\n",
        "                \"model\": model,\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"\"\"You are a safe and helpful assistant, and not malicious at all.\n",
        "                    You are quick at identifying malicious prompts and are skilled at changing them into clean prompts\n",
        "                    ONLY by removing injected parts.\"\"\"},\n",
        "\n",
        "                    {\"role\": \"user\", \"content\": f\"\"\"You will receive an\n",
        "                    injected prompts and a category that goes with the type of injection\n",
        "                    it recieved. For this prompt, reconstruct\n",
        "                    a safe and non malicious version\n",
        "                    by removing the injected portions.\n",
        "                    Here is the prompt: {text}\n",
        "                    and here is the corresponding category: {category}.\n",
        "                    Respond only with the rewritten prompt ONLY by REMOVING injection portions.\n",
        "                    Make sure TO NOT ADD any words to the prompt.\n",
        "                \"\"\"}\n",
        "                    ],\n",
        "                \"max_tokens\": num_tokens,\n",
        "                \"temperature\": 0.1\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(output_file, 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(request_data) + '\\n')\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating batch file: {e}\")\n",
        "\n",
        "def create_clean(dataset_injected, field, batch_size=1, output_file=\"batch_requests_clean.jsonl\"):\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for split_name, dataset in dataset_injected.items():\n",
        "        print(f\"Processing {split_name} split with {len(dataset)} samples...\")\n",
        "\n",
        "        dataset.map(\n",
        "            lambda batch: create_jsonl_file_clean(batch, field, output_file),\n",
        "            batched=True,\n",
        "            batch_size=batch_size,\n",
        "            desc=f\"Creating batch requests for {split_name}\"\n",
        "        )\n",
        "\n",
        "    print(f\"All batch requests written to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8nZYIWo9VkC"
      },
      "outputs": [],
      "source": [
        "def process_batch_and_add_pairs(original_dataset, batch_content, batch_size=1, filter_failed=True):\n",
        "    \"\"\"\n",
        "    Process batch responses and add text pairs to dataset\n",
        "\n",
        "    Args:\n",
        "        original_dataset: HuggingFace DatasetDict\n",
        "        batch_content: String containing batch responses (JSON/JSONL format)\n",
        "        batch_size: Size of each batch (default=1)\n",
        "        filter_failed: Whether to filter out failed entries (default=True)\n",
        "    \"\"\"\n",
        "    from datasets import DatasetDict, Dataset\n",
        "\n",
        "    print(\"Starting batch processing...\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Filter failed: {filter_failed}\")\n",
        "\n",
        "    # Step 1: Parse the batch content\n",
        "    print(\"Parsing batch responses...\")\n",
        "    batch_responses = {}\n",
        "\n",
        "    # Parse the batch content\n",
        "    batch_data = parse_batch_content(batch_content)\n",
        "\n",
        "    if not batch_data:\n",
        "        print(\"ERROR: No batch data could be parsed from the input!\")\n",
        "        print(f\"Input content preview: {repr(batch_content[:500] if batch_content else 'None')}...\")\n",
        "        return original_dataset\n",
        "\n",
        "    print(f\"Successfully parsed {len(batch_data)} batch responses\")\n",
        "\n",
        "    # Process each response\n",
        "    for response_data in batch_data:\n",
        "        try:\n",
        "            custom_id = response_data['custom_id']\n",
        "\n",
        "            # Extract content from response\n",
        "            content = response_data['response']['body']['choices'][0]['message']['content']\n",
        "\n",
        "            # Extract and clean the text pairs\n",
        "            text_pairs = []\n",
        "\n",
        "            # For batch_size=1, we expect a single text response\n",
        "            if batch_size == 1:\n",
        "                cleaned_text = extract_and_clean_text(content)\n",
        "                if cleaned_text:\n",
        "                    text_pairs = [cleaned_text]\n",
        "                else:\n",
        "                    print(f\"Empty or invalid text extracted from {custom_id}\")\n",
        "                    text_pairs = ['failed_extraction']\n",
        "            else:\n",
        "                # Handle multiple text pairs (split by lines or other delimiters)\n",
        "                content_lines = content.split('\\n')\n",
        "\n",
        "                for line_content in content_lines:\n",
        "                    cleaned_text = extract_and_clean_text(line_content)\n",
        "                    if cleaned_text:\n",
        "                        text_pairs.append(cleaned_text)\n",
        "\n",
        "                # Validate we have the expected number of text pairs\n",
        "                if len(text_pairs) != batch_size:\n",
        "                    print(f\"Expected {batch_size} text pairs for {custom_id}, but found {len(text_pairs)}\")\n",
        "                    print(f\"Text pairs found: {len(text_pairs)}\")\n",
        "                    print(f\"Raw content: {repr(content)}\")\n",
        "\n",
        "                    # Adjust text pairs list\n",
        "                    if len(text_pairs) < batch_size:\n",
        "                        missing = batch_size - len(text_pairs)\n",
        "                        text_pairs.extend(['failed_extraction'] * missing)\n",
        "                    else:\n",
        "                        text_pairs = text_pairs[:batch_size]\n",
        "\n",
        "            batch_responses[custom_id] = text_pairs\n",
        "            print(f\"Batch {custom_id}: Found {len(text_pairs)} text pairs\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing batch response: {e}\")\n",
        "            print(f\"Response data keys: {list(response_data.keys()) if isinstance(response_data, dict) else 'Not a dict'}\")\n",
        "            if isinstance(response_data, dict):\n",
        "                print(f\"Custom ID: {response_data.get('custom_id', 'MISSING')}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Found {len(batch_responses)} successful batches\")\n",
        "\n",
        "    # Step 2: Process each split\n",
        "    updated_splits = {}\n",
        "    failed_batches = set()\n",
        "\n",
        "    # First, let's see what custom_ids we actually have\n",
        "    available_custom_ids = sorted(batch_responses.keys())\n",
        "    print(f\"Available custom_ids: {available_custom_ids[:10]}...\" if len(available_custom_ids) > 10 else f\"Available custom_ids: {available_custom_ids}\")\n",
        "\n",
        "    # Extract just the batch numbers to understand the sequence\n",
        "    batch_numbers = []\n",
        "    for custom_id in available_custom_ids:\n",
        "        try:\n",
        "            parts = custom_id.split('-')\n",
        "            if len(parts) >= 2:\n",
        "                batch_num = int(parts[1])\n",
        "                batch_numbers.append(batch_num)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    if batch_numbers:\n",
        "        print(f\"Batch number range: {min(batch_numbers)} to {max(batch_numbers)} ({len(batch_numbers)} total)\")\n",
        "\n",
        "    for split_name, dataset in original_dataset.items():\n",
        "        print(f\"\\nProcessing {split_name}...\")\n",
        "\n",
        "        split_pairs = []\n",
        "        num_samples = len(dataset)\n",
        "\n",
        "        print(f\"Dataset has {num_samples} samples\")\n",
        "\n",
        "        # Calculate expected number of batches for this split\n",
        "        expected_batches = (num_samples + batch_size - 1) // batch_size  # Ceiling division\n",
        "        print(f\"Expected {expected_batches} batches for {num_samples} samples with batch_size={batch_size}\")\n",
        "\n",
        "        # Extract batch numbers from available responses to understand the numbering scheme\n",
        "        available_batch_nums = []\n",
        "        for custom_id in batch_responses.keys():\n",
        "            try:\n",
        "                parts = custom_id.split('-')\n",
        "                if len(parts) >= 2 and parts[1].isdigit():\n",
        "                    batch_num = int(parts[1])\n",
        "                    available_batch_nums.append(batch_num)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        available_batch_nums.sort()\n",
        "\n",
        "        if available_batch_nums:\n",
        "            print(f\"Available batch numbers: {available_batch_nums[0]} to {available_batch_nums[-1]} ({len(available_batch_nums)} total)\")\n",
        "            start_batch_num = available_batch_nums[0]\n",
        "            end_batch_num = available_batch_nums[-1]\n",
        "            expected_end_batch = start_batch_num + expected_batches - 1\n",
        "            print(f\"Expected batch range for this dataset: {start_batch_num} to {expected_end_batch}\")\n",
        "        else:\n",
        "            print(\"No valid batch numbers found in responses\")\n",
        "            split_pairs = ['api_failed'] * num_samples\n",
        "            failed_batches.update([f\"no-batch-nums\"])\n",
        "            continue\n",
        "\n",
        "        # Process samples in order, looking for the corresponding batch numbers\n",
        "        split_pairs = []\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            current_batch_size = min(batch_size, num_samples - i)\n",
        "            dataset_batch_index = i // batch_size  # 0, 1, 2, 3, ...\n",
        "\n",
        "            # Calculate expected batch number for this dataset position\n",
        "            expected_batch_num = start_batch_num + dataset_batch_index\n",
        "            expected_custom_id = f\"batch-{expected_batch_num}\"\n",
        "\n",
        "            # Look for this specific batch\n",
        "            if expected_custom_id in batch_responses:\n",
        "                batch_pairs = batch_responses[expected_custom_id]\n",
        "\n",
        "                # Ensure we have the right number of text pairs\n",
        "                if len(batch_pairs) == current_batch_size:\n",
        "                    split_pairs.extend(batch_pairs)\n",
        "                    print(f\"✓ Dataset position {dataset_batch_index} -> {expected_custom_id}: Added {len(batch_pairs)} text pairs\")\n",
        "                else:\n",
        "                    print(f\"⚠ Dataset position {dataset_batch_index} -> {expected_custom_id}: Expected {current_batch_size} text pairs, got {len(batch_pairs)}\")\n",
        "                    # Take what we have and fill the rest\n",
        "                    split_pairs.extend(batch_pairs[:current_batch_size])\n",
        "                    if len(batch_pairs) < current_batch_size:\n",
        "                        missing = current_batch_size - len(batch_pairs)\n",
        "                        split_pairs.extend(['failed_extraction'] * missing)\n",
        "            else:\n",
        "                # This specific batch is missing (failed at API level)\n",
        "                print(f\"✗ Dataset position {dataset_batch_index} -> {expected_custom_id}: Missing - marking as api_failed\")\n",
        "                split_pairs.extend(['api_failed'] * current_batch_size)\n",
        "                failed_batches.add(expected_custom_id)\n",
        "\n",
        "        # Validation\n",
        "        print(f\"Generated {len(split_pairs)} text pairs for {num_samples} samples\")\n",
        "\n",
        "        if len(split_pairs) != num_samples:\n",
        "            print(f\"ERROR: Text pair count mismatch!\")\n",
        "            print(f\"Expected: {num_samples}, Got: {len(split_pairs)}\")\n",
        "\n",
        "            if len(split_pairs) < num_samples:\n",
        "                missing = num_samples - len(split_pairs)\n",
        "                print(f\"Adding {missing} 'missing' entries\")\n",
        "                split_pairs.extend(['missing'] * missing)\n",
        "            elif len(split_pairs) > num_samples:\n",
        "                print(f\"Truncating to {num_samples} entries\")\n",
        "                split_pairs = split_pairs[:num_samples]\n",
        "\n",
        "        # Count text pair types\n",
        "        pair_counts = {}\n",
        "        for pair in split_pairs:\n",
        "            if pair in ['failed', 'failed_extraction', 'missing', 'api_failed']:\n",
        "                pair_type = pair\n",
        "            else:\n",
        "                pair_type = 'valid_text'\n",
        "            pair_counts[pair_type] = pair_counts.get(pair_type, 0) + 1\n",
        "\n",
        "        print(f\"Text pair distribution for {split_name}:\")\n",
        "        for pair_type, count in sorted(pair_counts.items()):\n",
        "            print(f\"  {pair_type}: {count}\")\n",
        "\n",
        "        # Add pair column\n",
        "        try:\n",
        "            dataset_with_pairs = dataset.add_column('pair', split_pairs)\n",
        "            updated_splits[split_name] = dataset_with_pairs\n",
        "            print(f\"✓ Successfully added text pairs to {split_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error adding text pairs to {split_name}: {e}\")\n",
        "            raise\n",
        "\n",
        "    # Create DatasetDict\n",
        "    dataset_dict = DatasetDict(updated_splits)\n",
        "\n",
        "    # Step 3: Apply filtering if requested\n",
        "    if filter_failed:\n",
        "        print(\"\\nApplying filtering to remove failed entries...\")\n",
        "        dataset_dict = filter_failed_extraction_datasetdict(dataset_dict)\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\nSUMMARY:\")\n",
        "    for split_name, split_dataset in dataset_dict.items():\n",
        "        print(f\"{split_name}: {len(split_dataset)} samples\")\n",
        "\n",
        "    if failed_batches:\n",
        "        print(f\"Failed batches: {sorted(failed_batches)}\")\n",
        "\n",
        "    return dataset_dict\n",
        "\n",
        "\n",
        "def parse_batch_content(batch_content):\n",
        "    \"\"\"\n",
        "    Robust parser for batch content (JSON/JSONL format)\n",
        "    \"\"\"\n",
        "    import json\n",
        "\n",
        "    if not batch_content:\n",
        "        print(\"Empty batch content received\")\n",
        "        return []\n",
        "\n",
        "    # Handle different input types\n",
        "    if hasattr(batch_content, 'text'):\n",
        "        batch_content = batch_content.text\n",
        "    elif not isinstance(batch_content, str):\n",
        "        batch_content = str(batch_content)\n",
        "\n",
        "    batch_content = batch_content.strip()\n",
        "\n",
        "    if not batch_content:\n",
        "        print(\"Empty batch content after processing\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Content length: {len(batch_content)} characters\")\n",
        "    print(f\"Content starts with: {repr(batch_content[:100])}\")\n",
        "    print(f\"Content ends with: {repr(batch_content[-100:])}\")\n",
        "\n",
        "    responses = []\n",
        "\n",
        "    try:\n",
        "        # Method 1: Try JSONL format (most common)\n",
        "        print(\"Attempting JSONL parsing...\")\n",
        "        lines = batch_content.split('\\n')\n",
        "        print(f\"Found {len(lines)} lines\")\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                response = json.loads(line)\n",
        "\n",
        "                # Validate response structure\n",
        "                if 'custom_id' not in response:\n",
        "                    print(f\"Line {i+1}: Missing custom_id\")\n",
        "                    continue\n",
        "\n",
        "                if 'response' not in response:\n",
        "                    print(f\"Line {i+1}: Missing response field\")\n",
        "                    continue\n",
        "\n",
        "                # Check for error field\n",
        "                if response.get('error'):\n",
        "                    print(f\"Line {i+1}: Response has error: {response['error']}\")\n",
        "                    continue\n",
        "\n",
        "                # Validate nested structure\n",
        "                try:\n",
        "                    content = response['response']['body']['choices'][0]['message']['content']\n",
        "                    responses.append(response)\n",
        "\n",
        "                    if i < 5:  # Show first few for debugging\n",
        "                        print(f\"✓ Line {i+1}: {response['custom_id']} -> '{content[:100]}...' ({len(content)} chars)\")\n",
        "\n",
        "                except (KeyError, IndexError, TypeError) as e:\n",
        "                    print(f\"Line {i+1}: Invalid response structure: {e}\")\n",
        "                    continue\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Line {i+1}: JSON decode error: {e}\")\n",
        "                if len(line) < 200:\n",
        "                    print(f\"  Full line: {repr(line)}\")\n",
        "                else:\n",
        "                    print(f\"  Line preview: {repr(line[:100])}...{repr(line[-100:])}\")\n",
        "                continue\n",
        "\n",
        "        if responses:\n",
        "            print(f\"Successfully parsed {len(responses)} responses from JSONL\")\n",
        "            return responses\n",
        "\n",
        "        # Method 2: Try JSON array format\n",
        "        print(\"JSONL failed, attempting JSON array parsing...\")\n",
        "        if batch_content.startswith('[') and batch_content.endswith(']'):\n",
        "            data = json.loads(batch_content)\n",
        "            if isinstance(data, list):\n",
        "                print(f\"Successfully parsed {len(data)} responses from JSON array\")\n",
        "                return data\n",
        "\n",
        "        # Method 3: Try single JSON object\n",
        "        print(\"Attempting single JSON object parsing...\")\n",
        "        data = json.loads(batch_content)\n",
        "        if isinstance(data, dict):\n",
        "            if 'responses' in data:\n",
        "                return data['responses']\n",
        "            elif 'data' in data:\n",
        "                return data['data']\n",
        "            else:\n",
        "                return [data]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"All parsing methods failed: {e}\")\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "def extract_and_clean_text(text):\n",
        "    \"\"\"\n",
        "    Extract and clean text content, preserving the full text but removing extra whitespace\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return None\n",
        "\n",
        "    # Strip leading and trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    # Replace multiple consecutive whitespace characters (including newlines) with single spaces\n",
        "    import re\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Final strip to ensure no leading/trailing spaces\n",
        "    cleaned_text = cleaned_text.strip()\n",
        "\n",
        "    return cleaned_text if cleaned_text else None\n",
        "\n",
        "\n",
        "def filter_failed_extraction_datasetdict(dataset_dict):\n",
        "    \"\"\"\n",
        "    Filter out failed entries from DatasetDict\n",
        "    \"\"\"\n",
        "    from datasets import DatasetDict\n",
        "\n",
        "    filtered_dict = {}\n",
        "    failure_types = {'failed', 'failed_extraction', 'missing', 'api_failed'}\n",
        "\n",
        "    for phase_name, dataset in dataset_dict.items():\n",
        "        print(f\"\\nFiltering {phase_name}...\")\n",
        "\n",
        "        # Show before filtering\n",
        "        before_count = len(dataset)\n",
        "        pair_counts = {}\n",
        "        for example in dataset:\n",
        "            pair = example['pair']\n",
        "            pair_type = 'valid_text' if pair not in failure_types else pair\n",
        "            pair_counts[pair_type] = pair_counts.get(pair_type, 0) + 1\n",
        "\n",
        "        print(f\"Before filtering ({before_count} samples):\")\n",
        "        for pair_type, count in sorted(pair_counts.items()):\n",
        "            print(f\"  {pair_type}: {count}\")\n",
        "\n",
        "        # Filter out failure types\n",
        "        filtered_dataset = dataset.filter(lambda example: example['pair'] not in failure_types)\n",
        "        filtered_dict[phase_name] = filtered_dataset\n",
        "\n",
        "        # Show after filtering\n",
        "        after_count = len(filtered_dataset)\n",
        "        print(f\"After filtering: {before_count} -> {after_count} ({before_count - after_count} removed)\")\n",
        "\n",
        "    return DatasetDict(filtered_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1KVcrd66Ub-"
      },
      "source": [
        "## Use OpenAI Batch API to create clean dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this section uses OpenAI's Batch API to create training pairs by generating cleaned versions of prompt injection attacks. The workflow splits classified datasets into chunks, creates batch requests asking GPT to remove malicious injection parts while preserving legitimate content, processes all chunks through the batch API, then concatenates results back together. The final dataset contains both original malicious prompts and their cleaned counterparts as training pairs for models to learn prompt injection removal."
      ],
      "metadata": {
        "id": "A_c6TqpTAYe-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqy7BavB75gE"
      },
      "outputs": [],
      "source": [
        "dataset_injected_first, dataset_injected_second, dataset_injected_third, dataset_injected_fourth = split_dataset_dict(updated_dataset_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66LD5tH08A6E"
      },
      "outputs": [],
      "source": [
        "dataset_injected_second1, dataset_injected_second2 = split_dataset_dict_half(dataset_injected_second)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxG0igpBtUT4"
      },
      "outputs": [],
      "source": [
        "create_clean(dataset_injected_first, \"body\", output_file=\"batch_requests_clean_first.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rwlLiCX51gk"
      },
      "outputs": [],
      "source": [
        "batch_input_file = client.files.create(\n",
        "    file=open(\"batch_requests_clean_first.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "\n",
        "print(batch_input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMFzybhy6CPS"
      },
      "outputs": [],
      "source": [
        "batch_input_file_id = batch_input_file.id\n",
        "batch_val = client.batches.create(\n",
        "    input_file_id=batch_input_file_id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"create clean first\"\n",
        "    }\n",
        ")\n",
        "print(batch_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKLr8PT66L27"
      },
      "outputs": [],
      "source": [
        "batch = client.batches.retrieve(\"batch_68881ad42820819097ab5632a4bea1dc\")\n",
        "print(batch)\n",
        "batch_output_file_id = batch.output_file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIsYoPyfL6my"
      },
      "outputs": [],
      "source": [
        "# Retrieve the batch using the ID you already have\n",
        "batch = client.batches.retrieve(\"batch_68881ad42820819097ab5632a4bea1dc\")\n",
        "print(f\"Batch status: {batch.status}\")\n",
        "\n",
        "# Check if the batch is completed\n",
        "if batch.status == \"completed\":\n",
        "    batch_output_file_id = batch.output_file_id\n",
        "    print(f\"batch_output_file_id = {batch_output_file_id}\")\n",
        "\n",
        "    # Now you can get the file content\n",
        "    file_response = client.files.content(batch_output_file_id)\n",
        "    print(file_response.text)\n",
        "\n",
        "elif batch.status == \"failed\":\n",
        "    print(\"Batch failed!\")\n",
        "    print(f\"Error details: {batch}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Batch is still {batch.status}. Please wait and try again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBIA6WLUNK_8"
      },
      "outputs": [],
      "source": [
        "file_response = client.files.content(batch_output_file_id)\n",
        "print(file_response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goxNBRlItp6t"
      },
      "outputs": [],
      "source": [
        "create_clean(dataset_injected_second1, \"body\", batch_size=1, output_file=\"batch_requests_clean_second1.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve8lU-Cxugfu"
      },
      "outputs": [],
      "source": [
        "batch_input_file_second1 = client.files.create(\n",
        "    file=open(\"batch_requests_clean_second1.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "\n",
        "print(batch_input_file_second1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyNRqPT0urLE"
      },
      "outputs": [],
      "source": [
        "batch_input_file_id_second1 = batch_input_file_second1.id\n",
        "batch_val_second1 = client.batches.create(\n",
        "    input_file_id=batch_input_file_id_second1,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"create clean second part 1\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_9bawG3uww4"
      },
      "outputs": [],
      "source": [
        "batch_second1 = client.batches.retrieve(\"batch_6888055fdfa88190b73f315cff6bf4d3\")\n",
        "print(batch_second1)\n",
        "batch_output_file_id_second1 = batch_second1.output_file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW1-x0j2vCfz"
      },
      "outputs": [],
      "source": [
        "file_response_second1 = client.files.content(batch_output_file_id_second1)\n",
        "print(file_response_second1.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03TcADPLvJgw"
      },
      "outputs": [],
      "source": [
        "create_clean(dataset_injected_second2, \"body\", output_file=\"batch_requests_clean_second2.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLAhQ7M4vUB8"
      },
      "outputs": [],
      "source": [
        "batch_input_file_second2 = client.files.create(\n",
        "    file=open(\"batch_requests_clean_second2.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "\n",
        "print(batch_input_file_second2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0FhenjfvVl6"
      },
      "outputs": [],
      "source": [
        "batch_input_file_id_second2 = batch_input_file_second2.id\n",
        "batch_val_second2 = client.batches.create(\n",
        "    input_file_id=batch_input_file_id_second2,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"create clean second part 2\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B98oIyH5vaws"
      },
      "outputs": [],
      "source": [
        "batch_second2 = client.batches.retrieve(\"batch_6888058ed9a88190b564a1ccec6ec333\")\n",
        "print(batch_second2)\n",
        "batch_output_file_id_second2 = batch_second2.output_file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yScT2k9vfo0"
      },
      "outputs": [],
      "source": [
        "file_response_second2 = client.files.content(batch_output_file_id_second2)\n",
        "print(file_response_second2.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO3piisLvgrV"
      },
      "outputs": [],
      "source": [
        "create_clean(dataset_injected_third, \"body\", output_file=\"batch_requests_clean_third.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oImmeJsvk0K"
      },
      "outputs": [],
      "source": [
        "batch_input_file_third = client.files.create(\n",
        "    file=open(\"batch_requests_clean_third.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "\n",
        "print(batch_input_file_third)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWegWKfVvsJd"
      },
      "outputs": [],
      "source": [
        "batch_input_file_id_third = batch_input_file_third.id\n",
        "batch_val_third = client.batches.create(\n",
        "    input_file_id=batch_input_file_id_third,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"create clean part three\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3feDS3sv3w8"
      },
      "outputs": [],
      "source": [
        "batch_third = client.batches.retrieve(\"batch_6888063b192081908f0cca7300a3e217\")\n",
        "print(batch_third)\n",
        "batch_output_file_id_third = batch_third.output_file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LVWi-lWv99C"
      },
      "outputs": [],
      "source": [
        "file_response_third = client.files.content(batch_output_file_id_third)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EFiIq-VwB9Z"
      },
      "outputs": [],
      "source": [
        "create_clean(dataset_injected_fourth, \"body\", output_file=\"batch_requests_clean_fourth.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wKKvQVlwOm8"
      },
      "outputs": [],
      "source": [
        "batch_input_file_fourth = client.files.create(\n",
        "    file=open(\"batch_requests_clean_fourth.jsonl\", \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "\n",
        "print(batch_input_file_fourth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF7h8YS9wT6i"
      },
      "outputs": [],
      "source": [
        "batch_input_file_id_fourth = batch_input_file_fourth.id\n",
        "batch_val_fourth = client.batches.create(\n",
        "    input_file_id=batch_input_file_id_fourth,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "        \"description\": \"create clean part four\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TK0JAdLwWjC"
      },
      "outputs": [],
      "source": [
        "batch_fourth = client.batches.retrieve(\"batch_688806be4608819087417496e9253385\")\n",
        "print(batch_fourth)\n",
        "batch_output_file_id_fourth = batch_fourth.output_file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvHzpdhIwYR8"
      },
      "outputs": [],
      "source": [
        "file_response_fourth = client.files.content(batch_output_file_id_fourth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8Pnig2q-IIf"
      },
      "outputs": [],
      "source": [
        "updated_dataset_part_one_clean = process_batch_and_add_pairs(\n",
        "     original_dataset=dataset_injected_first,\n",
        "     batch_content=file_response.text,\n",
        "     batch_size=1\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_Mf6m3v_2_Q"
      },
      "outputs": [],
      "source": [
        "updated_dataset_part_two1_clean = process_batch_and_add_pairs(\n",
        "     original_dataset=dataset_injected_second1,\n",
        "     batch_content=file_response_second1.text,\n",
        "     batch_size=1\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2RRQ2PMCDi8"
      },
      "outputs": [],
      "source": [
        "updated_dataset_part_two2_clean = process_batch_and_add_pairs(\n",
        "     original_dataset=dataset_injected_second2,\n",
        "     batch_content=file_response_second2.text,\n",
        "     batch_size=1\n",
        " )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2HAEKvaC89l"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "phase1 = concatenate_datasets([updated_dataset_part_two1_clean[\"Phase1\"], updated_dataset_part_two2_clean[\"Phase1\"]])\n",
        "phase2 = concatenate_datasets([updated_dataset_part_two1_clean[\"Phase2\"], updated_dataset_part_two2_clean[\"Phase2\"]])\n",
        "\n",
        "updated_dataset_part_two_clean = DatasetDict({\"Phase1\": phase1, \"Phase2\":phase2})\n",
        "updated_dataset_part_two_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDl5DGJTDJQD"
      },
      "outputs": [],
      "source": [
        "updated_dataset_part_three_clean = process_batch_and_add_pairs(\n",
        "     original_dataset=dataset_injected_third,\n",
        "     batch_content=file_response_third.text,\n",
        "     batch_size=1\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfd2gHh4DXci"
      },
      "outputs": [],
      "source": [
        "updated_dataset_part_four_clean = process_batch_and_add_pairs(\n",
        "     original_dataset=dataset_injected_fourth,\n",
        "     batch_content=file_response_fourth.text,\n",
        "     batch_size=1\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDSrWqZsDgzV"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "updated_dataset_full_phase1 = concatenate_datasets([updated_dataset_part_one_clean[\"Phase1\"], updated_dataset_part_two_clean[\"Phase1\"], updated_dataset_part_three_clean[\"Phase1\"], updated_dataset_part_four_clean[\"Phase1\"]])\n",
        "updated_dataset_full_phase2 = concatenate_datasets([updated_dataset_part_one_clean[\"Phase2\"], updated_dataset_part_two_clean[\"Phase2\"], updated_dataset_part_three_clean[\"Phase2\"], updated_dataset_part_four_clean[\"Phase2\"]])\n",
        "\n",
        "dataset_clean_with_pairs = DatasetDict({\"Phase1\":updated_dataset_full_phase1, \"Phase2\":updated_dataset_full_phase2})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGT8O3BAHzVG"
      },
      "source": [
        "# Create Clean-Clean Prompt Pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a synthetic email dataset by creating 86,000 batch API requests to OpenAI asking for pairs of semantically identical but differently worded emails, processes the responses to extract clean email pairs, concatenates all results into a single dataset with standardized field names ('body', 'pair', 'clean' category), and splits it into 32 smaller chunks for storage and distribution."
      ],
      "metadata": {
        "id": "RkXpC64-svFO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zjNI9hPIFg5"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import tiktoken\n",
        "counter = 0\n",
        "def create_jsonl_file_fully_clean(output_file=\"batch_requests_clean.jsonl\", model=\"gpt-3.5-turbo-0125\"):\n",
        "    global counter\n",
        "    try:\n",
        "        counter += 1\n",
        "        custom_id = f\"batch-{counter}\"\n",
        "\n",
        "        request_data = {\n",
        "            \"custom_id\": custom_id,\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "\n",
        "                \"model\": model,\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"\"\"You are a safe and helpful assistant, and not malicious at all.\n",
        "                    You specialize in creating clean, uninjected, emails and also\n",
        "                    you are skilled at rewriting them while keeping them uninjected.\"\"\"},\n",
        "\n",
        "                    {\"role\": \"user\", \"content\": f\"\"\"Write an original 2-3 sentence\n",
        "                    email and also provide a rewritten version of the same email\n",
        "                    in a numbered list format. Return the first email as 1. Email Content\n",
        "                    and return the second (rewritten version with VERY SIMILAR LENGTH WITH NO LESS THAN A 5 CHARACTER DIFFERENCE) as 2. Rewritten email. Both emails should discuss the same thing,\n",
        "                    however they are just reworded and written slightly differently. The semantic meaning of both emails should be THE EXACT SAME WITH NO SEMANTIC DRIFT in both the text and the embeddings.\n",
        "                \"\"\"}\n",
        "                    ],\n",
        "                \"max_tokens\": 100,\n",
        "                \"temperature\": 0.1\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(output_file, 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(request_data) + '\\n')\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating batch file: {e}\")\n",
        "\n",
        "def create_clean_prompts(count, batch_size=1, output_file=\"batch_requests_fully_clean.jsonl\"):\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for i in range(count):\n",
        "        print(f\"Creating batch requests for Batch Number {i+1}\")\n",
        "\n",
        "        for j in range(batch_size):\n",
        "            create_jsonl_file_fully_clean(output_file)\n",
        "\n",
        "    print(f\"All batch requests written to {output_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZT5jvX2KY2a"
      },
      "outputs": [],
      "source": [
        "d1 = create_clean_prompts(count=17200, output_file=\"batch_requests_fully_clean1.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3twfyJNL1R0"
      },
      "outputs": [],
      "source": [
        "d2 = create_clean_prompts(count=17200, output_file=\"batch_requests_fully_clean2.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7Z9x77UL2ug"
      },
      "outputs": [],
      "source": [
        "d3 = create_clean_prompts(count=17200, output_file=\"batch_requests_fully_clean3.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm_UxfRCL4KM"
      },
      "outputs": [],
      "source": [
        "d4 = create_clean_prompts(count=17200, output_file=\"batch_requests_fully_clean4.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U3R4jS3MAbX"
      },
      "outputs": [],
      "source": [
        "d5 = create_clean_prompts(count=17200, output_file=\"batch_requests_fully_clean5.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUm7lbAGM-0i"
      },
      "outputs": [],
      "source": [
        "def create_batch_job(client, file_path, description=\"batch job\", endpoint=\"/v1/chat/completions\", completion_window=\"24h\"):\n",
        "    try:\n",
        "        batch_input_file = client.files.create(\n",
        "            file=open(file_path, \"rb\"),\n",
        "            purpose=\"batch\"\n",
        "        )\n",
        "        print(f\"Uploaded file: {batch_input_file}\")\n",
        "\n",
        "        batch_val = client.batches.create(\n",
        "            input_file_id=batch_input_file.id,\n",
        "            endpoint=endpoint,\n",
        "            completion_window=completion_window,\n",
        "            metadata={\"description\": description}\n",
        "        )\n",
        "        print(f\"Created batch: {batch_val}\")\n",
        "\n",
        "        return batch_val.id\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating batch job: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def check_batch_status(client, batch_id):\n",
        "    try:\n",
        "        batch = client.batches.retrieve(batch_id)\n",
        "        print(f\"Batch status: {batch}\")\n",
        "\n",
        "        result = {\n",
        "            \"batch_id\": batch_id,\n",
        "            \"status\": batch.status,\n",
        "            \"created_at\": batch.created_at,\n",
        "            \"completed_at\": getattr(batch, 'completed_at', None),\n",
        "            \"failed_at\": getattr(batch, 'failed_at', None),\n",
        "            \"output_file_id\": getattr(batch, 'output_file_id', None),\n",
        "            \"error_file_id\": getattr(batch, 'error_file_id', None),\n",
        "            \"request_counts\": getattr(batch, 'request_counts', None)\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking batch status: {e}\")\n",
        "        return None\n",
        "\n",
        "def file_check(status):\n",
        "  file_response=None\n",
        "  if status[\"output_file_id\"] is not None:\n",
        "    print(\"Saving file response\")\n",
        "    file_response = client.files.content(status[\"output_file_id\"])\n",
        "  else:\n",
        "    print(\"File response not available\")\n",
        "  return file_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQxRItT0NuQM"
      },
      "outputs": [],
      "source": [
        "batch_id = create_batch_job(client, \"batch_requests_fully_clean1.jsonl\", \"create full clean first\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycWTbK0tN9qH"
      },
      "outputs": [],
      "source": [
        "status = check_batch_status(client, batch_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaRt1ALtPHnu"
      },
      "outputs": [],
      "source": [
        "file_response1 = file_check(status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GWnyb5hFyWc"
      },
      "outputs": [],
      "source": [
        "print(file_response1.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_3Njz53Y76O"
      },
      "outputs": [],
      "source": [
        "batch_id2 = create_batch_job(client, \"batch_requests_fully_clean2.jsonl\", \"create full clean second\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1147FpUPlHP"
      },
      "outputs": [],
      "source": [
        "status2 = check_batch_status(client, batch_id2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ78qwymPniL"
      },
      "outputs": [],
      "source": [
        "file_response2 = file_check(status2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQwpg66OP44m"
      },
      "outputs": [],
      "source": [
        "batch_id3 = create_batch_job(client, \"batch_requests_fully_clean3.jsonl\", \"create full clean third\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WdOmR4aP7-v"
      },
      "outputs": [],
      "source": [
        "status3 = check_batch_status(client, batch_id3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgH-0IRdQAMR"
      },
      "outputs": [],
      "source": [
        "file_response3 = file_check(status3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdLuem8LQC4q"
      },
      "outputs": [],
      "source": [
        "batch_id4 = create_batch_job(client, \"batch_requests_fully_clean4.jsonl\", \"create full clean third\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4kKoXXbQF7f"
      },
      "outputs": [],
      "source": [
        "status4 = check_batch_status(client, batch_id4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBogie9WQJam"
      },
      "outputs": [],
      "source": [
        "file_response4 = file_check(status4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4igzMCSQLik"
      },
      "outputs": [],
      "source": [
        "batch_id5 = create_batch_job(client, \"batch_requests_fully_clean5.jsonl\", \"create full clean fifth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PMpn3mlQQb3"
      },
      "outputs": [],
      "source": [
        "status5 = check_batch_status(client, batch_id5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG99QVLKQS8A"
      },
      "outputs": [],
      "source": [
        "file_response5 = file_check(status5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCrkosgcFwM3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset, DatasetDict\n",
        "from typing import Optional\n",
        "import re\n",
        "\n",
        "def parse_jsonl_to_dataset(jsonl_content: str, split_name: str = \"train\") -> DatasetDict:\n",
        "\n",
        "    data_records = []\n",
        "\n",
        "    lines = jsonl_content.strip().split('\\n')\n",
        "    for line_num, line in enumerate(lines):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "            try:\n",
        "                record = {}\n",
        "\n",
        "                # Extract basic identifiers\n",
        "                record[\"id\"] = obj.get(\"id\", f\"unknown_{line_num}\")\n",
        "                record[\"custom_id\"] = obj.get(\"custom_id\", \"\")\n",
        "\n",
        "                # Skip if there's an error\n",
        "                if obj.get(\"error\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract response data\n",
        "                response = obj.get(\"response\", {})\n",
        "                if response.get(\"status_code\") != 200:\n",
        "                    continue\n",
        "\n",
        "                body = response.get(\"body\", {})\n",
        "                choices = body.get(\"choices\", [])\n",
        "\n",
        "                if choices:\n",
        "                    # Extract assistant response content\n",
        "                    message = choices[0].get(\"message\", {})\n",
        "                    content = message.get(\"content\", \"\")\n",
        "\n",
        "                    original_email, rewritten_email = extract_email_pair(content)\n",
        "\n",
        "                    # Only store records where both emails were successfully extracted\n",
        "                    if original_email and rewritten_email:\n",
        "                        record[\"original_email\"] = original_email\n",
        "                        record[\"rewritten_email\"] = rewritten_email\n",
        "                        record[\"model\"] = body.get(\"model\", \"\")\n",
        "                        record[\"finish_reason\"] = choices[0].get(\"finish_reason\", \"\")\n",
        "                        data_records.append(record)\n",
        "\n",
        "            except Exception:\n",
        "                continue\n",
        "        except:\n",
        "          continue\n",
        "    dataset = Dataset.from_list(data_records)\n",
        "    return DatasetDict({split_name: dataset})\n",
        "\n",
        "\n",
        "def extract_email_pair(content: str) -> tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Extract original and rewritten email content, ensuring no numbered prefixes are included.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Primary pattern: Look for numbered sections with headers\n",
        "        pattern1 = r\"1\\.\\s*(?:Email Content|Original Email|Original):\\s*\\n(.*?)(?=2\\.\\s*(?:Rewritten email|Rewritten Email|Rewritten):|$)\"\n",
        "        pattern2 = r\"2\\.\\s*(?:Rewritten email|Rewritten Email|Rewritten):\\s*\\n(.*?)$\"\n",
        "\n",
        "        match1 = re.search(pattern1, content, re.DOTALL | re.IGNORECASE)\n",
        "        match2 = re.search(pattern2, content, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "        if match1 and match2:\n",
        "            original_email = clean_email_content(match1.group(1))\n",
        "            rewritten_email = clean_email_content(match2.group(1))\n",
        "            return original_email, rewritten_email\n",
        "\n",
        "        # Fallback: Look for Subject: patterns\n",
        "        subjects = re.findall(r'Subject:.*?(?=Subject:|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "        if len(subjects) >= 2:\n",
        "            original_email = clean_email_content(subjects[0])\n",
        "            rewritten_email = clean_email_content(subjects[1])\n",
        "            return original_email, rewritten_email\n",
        "\n",
        "        # Another fallback: Split by double newlines followed by Subject:\n",
        "        sections = re.split(r'\\n\\s*\\n(?=Subject:)', content, flags=re.IGNORECASE)\n",
        "        if len(sections) >= 2:\n",
        "            email_sections = [s.strip() for s in sections if 'subject:' in s.lower()]\n",
        "            if len(email_sections) >= 2:\n",
        "                original_email = clean_email_content(email_sections[0])\n",
        "                rewritten_email = clean_email_content(email_sections[1])\n",
        "                return original_email, rewritten_email\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting email pair: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def clean_email_content(email_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean email content by removing numbered prefixes that appear at the start of the content.\n",
        "    \"\"\"\n",
        "    if not email_text:\n",
        "        return email_text\n",
        "\n",
        "    cleaned = email_text.strip()\n",
        "\n",
        "    # Remove section headers like \"1. Email Content:\" or \"2. Rewritten email:\" at the start\n",
        "    cleaned = re.sub(r'^\\s*[12]\\.\\s*(?:Email Content|Original Email|Original|Rewritten email|Rewritten Email|Rewritten):\\s*\\n?', '', cleaned, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove any numbered prefix at the very start of the content (like \"1. Subject:\" -> \"Subject:\")\n",
        "    cleaned = re.sub(r'^\\s*\\d+\\.\\s+', '', cleaned)\n",
        "\n",
        "    # Clean up extra whitespace\n",
        "    cleaned = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', cleaned)  # Multiple blank lines to double\n",
        "    cleaned = cleaned.strip()\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def save_dataset(dataset_dict: DatasetDict, output_path: str):\n",
        "    dataset_dict.save_to_disk(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv_kgLlMInRB"
      },
      "outputs": [],
      "source": [
        "dataset_dict1 = parse_jsonl_to_dataset(file_response1.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCUpSO2XI4EC"
      },
      "outputs": [],
      "source": [
        "dataset_dict2 = parse_jsonl_to_dataset(file_response2.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAPsJfCTI8Ki"
      },
      "outputs": [],
      "source": [
        "dataset_dict3 = parse_jsonl_to_dataset(file_response3.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0050WdfSKanI"
      },
      "outputs": [],
      "source": [
        "dataset_dict4 = parse_jsonl_to_dataset(file_response4.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCind7OgKbmk"
      },
      "outputs": [],
      "source": [
        "dataset_dict5 = parse_jsonl_to_dataset(file_response5.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKjFiHBvKnB2"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets, DatasetDict\n",
        "\n",
        "dict1 = concatenate_datasets([dataset_dict1[\"train\"], dataset_dict2[\"train\"],dataset_dict3[\"train\"],dataset_dict4[\"train\"], dataset_dict5[\"train\"]])\n",
        "full_dataset_dict = DatasetDict({\"train\":dict1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABHG8Y9TLG8k"
      },
      "outputs": [],
      "source": [
        "full_dataset_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-CDQDr0_Edo"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict, Dataset\n",
        "\n",
        "def transform_dataset(full_dataset_dict):\n",
        "\n",
        "    def process_split(dataset):\n",
        "        # Create new dataset with transformed structure\n",
        "        new_data = {\n",
        "            'id': dataset['id'],\n",
        "            'custom_id': dataset['custom_id'],\n",
        "            'body': dataset['original_email'],  # Rename original_email to body\n",
        "            'pair': dataset['rewritten_email'],  # Rename rewritten_email to pair\n",
        "            'category': ['clean'] * len(dataset)  # Add 'clean' category to every element\n",
        "        }\n",
        "\n",
        "        return Dataset.from_dict(new_data)\n",
        "\n",
        "    # Transform each split in the dataset\n",
        "    transformed_dict = {}\n",
        "    for split_name, dataset in full_dataset_dict.items():\n",
        "        transformed_dict[split_name] = process_split(dataset)\n",
        "\n",
        "    return DatasetDict(transformed_dict)\n",
        "\n",
        "\n",
        "full_dataset_dict = transform_dataset(full_dataset_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ5KEio9MnIg"
      },
      "outputs": [],
      "source": [
        "full_dataset_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "import multiprocessing as mp\n",
        "from functools import partial\n",
        "import gc\n",
        "\n",
        "def process_batch_on_gpu(batch_data, features, device):\n",
        "    \"\"\"Process a batch of data on GPU for faster operations\"\"\"\n",
        "    try:\n",
        "        batch_rows = []\n",
        "\n",
        "        # Move batch to GPU if it's not already there\n",
        "        if isinstance(batch_data, dict):\n",
        "            gpu_batch = {}\n",
        "            for feature in features:\n",
        "                if feature in batch_data:\n",
        "                    data = batch_data[feature]\n",
        "                    if torch.is_tensor(data):\n",
        "                        gpu_batch[feature] = data.to(device) if data.device != device else data\n",
        "                    else:\n",
        "                        # Convert to tensor and move to GPU if numeric\n",
        "                        try:\n",
        "                            if isinstance(data, (list, np.ndarray)):\n",
        "                                gpu_batch[feature] = torch.tensor(data, device=device)\n",
        "                            else:\n",
        "                                gpu_batch[feature] = data\n",
        "                        except:\n",
        "                            gpu_batch[feature] = data\n",
        "                else:\n",
        "                    gpu_batch[feature] = None\n",
        "\n",
        "            # Convert back to CPU for JSON serialization\n",
        "            cpu_row = {}\n",
        "            for feature in features:\n",
        "                if torch.is_tensor(gpu_batch[feature]):\n",
        "                    cpu_row[feature] = gpu_batch[feature].cpu().tolist()\n",
        "                elif isinstance(gpu_batch[feature], np.ndarray):\n",
        "                    cpu_row[feature] = gpu_batch[feature].tolist()\n",
        "                else:\n",
        "                    cpu_row[feature] = gpu_batch[feature]\n",
        "\n",
        "            batch_rows.append(cpu_row)\n",
        "\n",
        "        return batch_rows\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"GPU processing failed, falling back to CPU: {e}\")\n",
        "        # Fallback to CPU processing\n",
        "        batch_rows = []\n",
        "        for feature in features:\n",
        "            row = {}\n",
        "            for feat in features:\n",
        "                if isinstance(batch_data, dict) and feat in batch_data:\n",
        "                    data = batch_data[feat]\n",
        "                    if torch.is_tensor(data):\n",
        "                        row[feat] = data.cpu().tolist()\n",
        "                    elif isinstance(data, np.ndarray):\n",
        "                        row[feat] = data.tolist()\n",
        "                    else:\n",
        "                        row[feat] = data\n",
        "                else:\n",
        "                    row[feat] = None\n",
        "            batch_rows.append(row)\n",
        "        return batch_rows\n",
        "\n",
        "def process_dataset_parallel(dataset, phase_name, batch_size=1000, use_gpu=True, num_workers=None):\n",
        "    \"\"\"Process dataset in parallel batches with optional GPU acceleration\"\"\"\n",
        "\n",
        "    # Setup device\n",
        "    if use_gpu and torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"Using CPU for processing\")\n",
        "\n",
        "    # Set number of workers\n",
        "    if num_workers is None:\n",
        "        num_workers = min(4, mp.cpu_count())  # Conservative for memory\n",
        "\n",
        "    features = list(dataset.features.keys())\n",
        "    total_rows = len(dataset)\n",
        "\n",
        "    print(f\"Processing {phase_name} with {num_workers} workers in batches of {batch_size}...\")\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    # Process in batches\n",
        "    for start_idx in tqdm(range(0, total_rows, batch_size), desc=f\"Processing {phase_name}\"):\n",
        "        end_idx = min(start_idx + batch_size, total_rows)\n",
        "\n",
        "        # Get batch data\n",
        "        batch_indices = list(range(start_idx, end_idx))\n",
        "        batch_data = dataset.select(batch_indices)\n",
        "\n",
        "        # Convert batch to format suitable for GPU processing\n",
        "        if use_gpu and torch.cuda.is_available():\n",
        "            try:\n",
        "                # Try to use GPU format if available\n",
        "                batch_data.set_format(type='torch', device=device)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Process batch (could be parallelized further if needed)\n",
        "        batch_rows = []\n",
        "        for i in range(len(batch_data)):\n",
        "            row = {}\n",
        "            for feature in features:\n",
        "                data = batch_data[i][feature]\n",
        "                if torch.is_tensor(data):\n",
        "                    row[feature] = data.cpu().tolist() if data.is_cuda else data.tolist()\n",
        "                elif isinstance(data, np.ndarray):\n",
        "                    row[feature] = data.tolist()\n",
        "                else:\n",
        "                    row[feature] = data\n",
        "            batch_rows.append(row)\n",
        "\n",
        "        all_data.extend(batch_rows)\n",
        "\n",
        "        # Clear GPU cache periodically\n",
        "        if use_gpu and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        'features': features,\n",
        "        'num_rows': len(all_data),\n",
        "        'data': all_data\n",
        "    }\n",
        "\n",
        "def save_data(dataset_dict, filename='dataset.json', output_dir='/content/drive/MyDrive/Algoverse/',\n",
        "              mount_drive=True, use_gpu=True, batch_size=1000, num_workers=None, use_parallel_phases=True):\n",
        "    \"\"\"\n",
        "    Save dataset with GPU acceleration and parallel processing\n",
        "\n",
        "    Args:\n",
        "        dataset_dict: Dictionary of datasets to save\n",
        "        filename: Output filename\n",
        "        output_dir: Output directory\n",
        "        mount_drive: Whether to mount Google Drive\n",
        "        use_gpu: Whether to use GPU acceleration\n",
        "        batch_size: Batch size for processing\n",
        "        num_workers: Number of parallel workers\n",
        "        use_parallel_phases: Whether to process phases in parallel\n",
        "    \"\"\"\n",
        "\n",
        "    if mount_drive:\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Check GPU availability\n",
        "    if use_gpu and torch.cuda.is_available():\n",
        "        print(f\"GPU acceleration enabled: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "        # Clear cache at start\n",
        "        torch.cuda.empty_cache()\n",
        "    else:\n",
        "        print(\"Using CPU processing\")\n",
        "        use_gpu = False\n",
        "\n",
        "    # Set number of workers\n",
        "    if num_workers is None:\n",
        "        num_workers = min(4, mp.cpu_count())\n",
        "\n",
        "    json_data = {}\n",
        "\n",
        "    if use_parallel_phases and len(dataset_dict) > 1:\n",
        "        # Process phases in parallel\n",
        "        print(f\"Processing {len(dataset_dict)} phases in parallel...\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=min(len(dataset_dict), num_workers)) as executor:\n",
        "            # Submit all phase processing tasks\n",
        "            future_to_phase = {\n",
        "                executor.submit(process_dataset_parallel, dataset, phase_name, batch_size, use_gpu, 1): phase_name\n",
        "                for phase_name, dataset in dataset_dict.items()\n",
        "            }\n",
        "\n",
        "            # Collect results\n",
        "            for future in tqdm(future_to_phase, desc=\"Processing phases\"):\n",
        "                phase_name = future_to_phase[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    json_data[phase_name] = result\n",
        "                    print(f\"Completed {phase_name}: {result['num_rows']} rows\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {phase_name}: {e}\")\n",
        "    else:\n",
        "        # Process phases sequentially\n",
        "        for phase_name, dataset in dataset_dict.items():\n",
        "            result = process_dataset_parallel(dataset, phase_name, batch_size, use_gpu, num_workers)\n",
        "            json_data[phase_name] = result\n",
        "            print(f\"Completed {phase_name}: {result['num_rows']} rows\")\n",
        "\n",
        "    # Save to file\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "    print(f\"Saving data to {output_path}...\")\n",
        "\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        print(f\"Successfully saved dataset to {output_path}\")\n",
        "        print(f\"File size: {os.path.getsize(output_path) / 1e6:.1f} MB\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "        # Try saving without indentation to save space\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(json_data, f, ensure_ascii=False, default=str)\n",
        "        print(f\"Saved without formatting due to memory constraints\")\n",
        "\n",
        "    # Clean up GPU memory\n",
        "    if use_gpu and torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "aRgLrzXXlfIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset_dict, number=32):\n",
        "    result = []\n",
        "    counter = 0\n",
        "    for phase_name, dataset in dataset_dict.items():\n",
        "        res = {}\n",
        "        total_rows = len(dataset)\n",
        "        quarter = total_rows // number\n",
        "\n",
        "        for i in range(number):\n",
        "          if i==(number-1):\n",
        "              split_val = dataset.select(range(quarter*i, len(dataset)))\n",
        "          else:\n",
        "              split_val = dataset.select(range(quarter*i, quarter*(i+1)))\n",
        "          res[phase_name] = split_val\n",
        "\n",
        "          result.append(DatasetDict(res))\n",
        "\n",
        "    for dataset_dict in result:\n",
        "        save_data(dataset_dict, filename=\"token_\"+str(counter))\n",
        "        counter+=1\n"
      ],
      "metadata": {
        "id": "b67RCqvGkcu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6W97k973mkL"
      },
      "outputs": [],
      "source": [
        "batches = split_dataset(dataset_clean_with_pairs)\n",
        "batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9td7G0ePBlFB"
      },
      "source": [
        "# Tokenization and Embedding Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this section includes data pipeline with GPU-accelerated loading, stratified sampling, and dataset combination functions. It reconstructs datasets from JSON files, maintains category distributions when creating balanced subsets from prompt injection and clean email data, and combines them into training/test datasets with appropriate similarity scores. The pipeline includes parallel processing for efficient data handling and concludes with Hugging Face authentication for uploading the processed datasets to the Huggingface Hub.\n"
      ],
      "metadata": {
        "id": "UHD8Ps8yAkft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGqob71eNXsP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset, DatasetDict\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "def load_data(json_file_path, mount_drive=True):\n",
        "  if mount_drive:\n",
        "        drive.mount('/content/drive')\n",
        "  print(f\"Loading data from {json_file_path}...\")\n",
        "  with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "      json_data = json.load(f)\n",
        "\n",
        "  dataset_dict = {}\n",
        "  for phase_name, phase_info in json_data.items():\n",
        "      print(f\"Processing {phase_name}...\")\n",
        "\n",
        "      dataset_dict[phase_name] = Dataset.from_list(phase_info['data'])\n",
        "\n",
        "  tokenized_dataset_dict = DatasetDict(dataset_dict)\n",
        "\n",
        "  print(\"Successfully loaded DatasetDict!\")\n",
        "\n",
        "  return tokenized_dataset_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obJLA2sqwqwY"
      },
      "outputs": [],
      "source": [
        "full_dataset_dict[\"train\"]=full_dataset_dict[\"train\"].add_column(\"similarity\", [1.0] * len(full_dataset_dict[\"train\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAafvdYeyDbq"
      },
      "outputs": [],
      "source": [
        "full_dataset_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rAt-4WyeTKB"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets, DatasetDict\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import multiprocessing as mp\n",
        "\n",
        "def load_and_combine_datasets(use_gpu=True, num_workers=None):\n",
        "    \"\"\"Load datasets from token_0 to token_31 and combine them into one large dataset\n",
        "\n",
        "    Args:\n",
        "        use_gpu (bool): Whether to use GPU for operations when possible\n",
        "        num_workers (int): Number of parallel workers for loading (None = auto-detect)\n",
        "    \"\"\"\n",
        "\n",
        "    # Check GPU availability\n",
        "    if use_gpu and torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"GPU detected: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"Using CPU for operations\")\n",
        "\n",
        "    # Set number of workers for parallel loading\n",
        "    if num_workers is None:\n",
        "        num_workers = min(8, mp.cpu_count())  # Reasonable default\n",
        "\n",
        "    all_datasets = []\n",
        "    base_path = \"/content/drive/MyDrive/Algoverse/token_\"\n",
        "\n",
        "    print(f\"Loading datasets with {num_workers} parallel workers...\")\n",
        "\n",
        "    def load_single_dataset(i):\n",
        "        \"\"\"Helper function to load a single dataset\"\"\"\n",
        "        dataset_path = f\"{base_path}{i}\"\n",
        "        try:\n",
        "            dataset = load_data(dataset_path)\n",
        "            return i, dataset\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load dataset {i}: {e}\")\n",
        "            return i, None\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel loading\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        # Submit all loading tasks\n",
        "        futures = [executor.submit(load_single_dataset, i) for i in range(32)]\n",
        "\n",
        "        # Collect results as they complete\n",
        "        for future in futures:\n",
        "            i, dataset = future.result()\n",
        "            if dataset is not None:\n",
        "                print(f\"Loaded dataset {i}\")\n",
        "\n",
        "                # Handle different dataset structures\n",
        "                if isinstance(dataset, DatasetDict):\n",
        "                    for split_name, split_data in dataset.items():\n",
        "                        print(f\"  Split '{split_name}': {len(split_data)} rows\")\n",
        "                        # Set format for GPU if available and data supports it\n",
        "                        if use_gpu and torch.cuda.is_available():\n",
        "                            try:\n",
        "                                split_data.set_format(type='torch', device=device)\n",
        "                            except:\n",
        "                                pass  # Some datasets might not support torch format\n",
        "                        all_datasets.append(split_data)\n",
        "                else:\n",
        "                    print(f\"  Dataset: {len(dataset)} rows\")\n",
        "                    # Set format for GPU if available\n",
        "                    if use_gpu and torch.cuda.is_available():\n",
        "                        try:\n",
        "                            dataset.set_format(type='torch', device=device)\n",
        "                        except:\n",
        "                            pass\n",
        "                    all_datasets.append(dataset)\n",
        "\n",
        "    if not all_datasets:\n",
        "        raise ValueError(\"No datasets were successfully loaded!\")\n",
        "\n",
        "    # Combine all datasets (this operation is CPU-bound)\n",
        "    print(f\"\\nCombining {len(all_datasets)} dataset splits...\")\n",
        "    combined_dataset = concatenate_datasets(all_datasets)\n",
        "\n",
        "    # Set format for GPU on final dataset if requested\n",
        "    if use_gpu and torch.cuda.is_available():\n",
        "        try:\n",
        "            combined_dataset.set_format(type='torch', device=device)\n",
        "            print(f\"Dataset moved to GPU: {device}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not move dataset to GPU: {e}\")\n",
        "\n",
        "    # Create a DatasetDict with train split\n",
        "    final_dataset = DatasetDict({\n",
        "        'train': combined_dataset\n",
        "    })\n",
        "\n",
        "    print(f\"Successfully combined all datasets!\")\n",
        "    print(f\"Final dataset shape: {len(combined_dataset)} rows\")\n",
        "    print(f\"Features: {list(combined_dataset.features.keys())}\")\n",
        "\n",
        "    return final_dataset\n",
        "\n",
        "combined_dataset = load_and_combine_datasets(use_gpu=True, num_workers=8)\n",
        "\n",
        "print(f\"\\nFinal combined dataset:\")\n",
        "print(combined_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RSvDgooeg-5"
      },
      "outputs": [],
      "source": [
        "combined_dataset[\"train\"] = combined_dataset[\"train\"].add_column(\"similarity\", [0.0] * len(combined_dataset[\"train\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkGN_wv7sHNO"
      },
      "outputs": [],
      "source": [
        "combined_dataset[\"train\"] = combined_dataset[\"train\"].remove_columns([\"body_embeddings\", \"pair_embeddings\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSiVoMACIOCm"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "def filter_dataset_columns(dataset, keep_columns=None):\n",
        "    \"\"\"\n",
        "    Filter dataset to keep only specified columns.\n",
        "    Works with both Dataset and DatasetDict objects.\n",
        "\n",
        "    Args:\n",
        "        dataset: The input dataset (Dataset or DatasetDict)\n",
        "        keep_columns: List of column names to keep (default: ['body', 'pair', 'body_embeddings', 'pair_embeddings'])\n",
        "\n",
        "    Returns:\n",
        "        New filtered dataset with only the specified columns (same type as input)\n",
        "    \"\"\"\n",
        "\n",
        "    if keep_columns is None:\n",
        "        keep_columns = ['body', 'pair', 'body_embeddings', 'pair_embeddings']\n",
        "\n",
        "    # Handle DatasetDict\n",
        "    if isinstance(dataset, DatasetDict):\n",
        "        filtered_dict = {}\n",
        "\n",
        "        for split_name, split_dataset in dataset.items():\n",
        "            print(f\"\\nProcessing split: {split_name}\")\n",
        "\n",
        "            # Check which columns exist in this split\n",
        "            available_columns = split_dataset.column_names\n",
        "            columns_to_keep = [col for col in keep_columns if col in available_columns]\n",
        "\n",
        "            print(f\"Original columns: {available_columns}\")\n",
        "            print(f\"Requested columns: {keep_columns}\")\n",
        "            print(f\"Columns to keep (available): {columns_to_keep}\")\n",
        "\n",
        "            missing_columns = [col for col in keep_columns if col not in available_columns]\n",
        "            if missing_columns:\n",
        "                print(f\"Warning: These requested columns don't exist: {missing_columns}\")\n",
        "\n",
        "            # Filter this split\n",
        "            filtered_dict[split_name] = split_dataset.select_columns(columns_to_keep)\n",
        "\n",
        "        return DatasetDict(filtered_dict)\n",
        "\n",
        "    # Handle single Dataset\n",
        "    else:\n",
        "        # Check which columns exist in the dataset\n",
        "        available_columns = dataset.column_names\n",
        "        columns_to_keep = [col for col in keep_columns if col in available_columns]\n",
        "\n",
        "        # Print info about what we're keeping vs what's missing\n",
        "        print(f\"Original columns: {available_columns}\")\n",
        "        print(f\"Requested columns: {keep_columns}\")\n",
        "        print(f\"Columns to keep (available): {columns_to_keep}\")\n",
        "\n",
        "        missing_columns = [col for col in keep_columns if col not in available_columns]\n",
        "        if missing_columns:\n",
        "            print(f\"Warning: These requested columns don't exist: {missing_columns}\")\n",
        "\n",
        "        # Create and return a new filtered dataset\n",
        "        filtered_dataset = dataset.select_columns(columns_to_keep)\n",
        "\n",
        "        return filtered_dataset\n",
        "\n",
        "# Example usage:\n",
        "def process_dataset(input_dataset):\n",
        "    \"\"\"\n",
        "    Process a dataset by filtering to keep only specific columns.\n",
        "    Works with both Dataset and DatasetDict objects.\n",
        "\n",
        "    Args:\n",
        "        input_dataset: The dataset to process (Dataset or DatasetDict)\n",
        "\n",
        "    Returns:\n",
        "        New filtered dataset (same type as input)\n",
        "    \"\"\"\n",
        "    return filter_dataset_columns(\n",
        "        input_dataset,\n",
        "        keep_columns=['body', 'pair', \"category\", \"similarity\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uDo4n4LIZ7Q"
      },
      "outputs": [],
      "source": [
        "filtered_dataset = process_dataset(combined_dataset)\n",
        "filtered_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiCmC52lRqHs"
      },
      "outputs": [],
      "source": [
        "filtered_clean = process_dataset(full_dataset_dict)\n",
        "filtered_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVEaVVkv7YKF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "import numpy as np\n",
        "\n",
        "def sample_dataset_stratified(dataset_dict, sample_fraction=0.5, category_column='category', random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    sampled_dict = {}\n",
        "    remaining_dict = {}\n",
        "\n",
        "    for split_name, dataset in dataset_dict.items():\n",
        "        df = dataset.to_pandas()\n",
        "        original_counts = df[category_column].value_counts()\n",
        "        target_size = int(len(df) * sample_fraction)\n",
        "\n",
        "        print(f\"{split_name}: {len(df)} → {target_size} rows sampled, {len(df) - target_size} rows remaining\")\n",
        "\n",
        "        sampled_dfs = []\n",
        "        remaining_dfs = []\n",
        "\n",
        "        for category in original_counts.index:\n",
        "            category_df = df[df[category_column] == category]\n",
        "            category_sample_size = max(1, int(len(category_df) * sample_fraction))\n",
        "\n",
        "            sampled_category = category_df.sample(\n",
        "                n=min(category_sample_size, len(category_df)),\n",
        "                random_state=random_state\n",
        "            )\n",
        "\n",
        "            # Get remaining data by excluding sampled indices\n",
        "            remaining_category = category_df.drop(sampled_category.index)\n",
        "\n",
        "            sampled_dfs.append(sampled_category)\n",
        "            remaining_dfs.append(remaining_category)\n",
        "\n",
        "        # Combine and shuffle sampled data\n",
        "        sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n",
        "        sampled_df = sampled_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "        sampled_dict[split_name] = Dataset.from_pandas(sampled_df)\n",
        "\n",
        "        # Combine and shuffle remaining data\n",
        "        remaining_df = pd.concat(remaining_dfs, ignore_index=True)\n",
        "        remaining_df = remaining_df.sample(frac=1, random_state=random_state + 1).reset_index(drop=True)\n",
        "        remaining_dict[split_name] = Dataset.from_pandas(remaining_df)\n",
        "\n",
        "    return DatasetDict(sampled_dict), DatasetDict(remaining_dict)\n",
        "\n",
        "def verify_distribution(original_dict, sampled_dict, remaining_dict=None, category_column='category'):\n",
        "    print(\"\\nDistribution Verification:\")\n",
        "    for split_name in original_dict.keys():\n",
        "        original_df = original_dict[split_name].to_pandas()\n",
        "        sampled_df = sampled_dict[split_name].to_pandas()\n",
        "\n",
        "        original_dist = original_df[category_column].value_counts(normalize=True)\n",
        "        sampled_dist = sampled_df[category_column].value_counts(normalize=True)\n",
        "\n",
        "        print(f\"\\n{split_name}:\")\n",
        "        print(f\"  Original: {len(original_df)} rows\")\n",
        "        print(f\"  Sampled:  {len(sampled_df)} rows\")\n",
        "\n",
        "        if remaining_dict:\n",
        "            remaining_df = remaining_dict[split_name].to_pandas()\n",
        "            remaining_dist = remaining_df[category_column].value_counts(normalize=True)\n",
        "            print(f\"  Remaining: {len(remaining_df)} rows\")\n",
        "\n",
        "        print(\"  Category distributions:\")\n",
        "        for category in original_dist.index:\n",
        "            orig_pct = original_dist[category] * 100\n",
        "            samp_pct = sampled_dist.get(category, 0) * 100\n",
        "            print(f\"    {category}: Original {orig_pct:.1f}% → Sampled {samp_pct:.1f}%\", end=\"\")\n",
        "\n",
        "            if remaining_dict:\n",
        "                rem_pct = remaining_dist.get(category, 0) * 100\n",
        "                print(f\" | Remaining {rem_pct:.1f}%\")\n",
        "            else:\n",
        "                print()\n",
        "\n",
        "sampled_dataset, remaining_dataset = sample_dataset_stratified(filtered_dataset, sample_fraction=0.5)\n",
        "verify_distribution(filtered_dataset, sampled_dataset, remaining_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iV1Z8YUzSzk"
      },
      "outputs": [],
      "source": [
        "s1, s2 = sample_dataset_stratified(sampled_dataset, sample_fraction=0.7)\n",
        "t1, t2 = sample_dataset_stratified(filtered_clean, sample_fraction=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch1wpCKf6Y8u"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets, DatasetDict\n",
        "train_dataset = DatasetDict({\"train\":concatenate_datasets([s1[\"train\"], t1[\"train\"]])})\n",
        "test_dataset = DatasetDict({\"train\":concatenate_datasets([s2[\"train\"], t2[\"train\"]])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXZJAgpmexzy"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "login(userdata.get(\"HUGGINGFACE_TOKEN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "0jeI8fGnD9ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section implements four fine-tuning pipelines for sentence similarity models using Sentence BERT All-MPNet v2, LLaMA-3-8B, Mistral-7B, and Qwen2-7B. The code sets up RunPod cloud environments with memory optimization, converts large language models to SentenceTransformer-compatible versions using LoRA and 4-bit quantization, and trains on sentence pairs with cosine similarity loss. All pipelines follow the same framework but differ in base model architecture and configuration parameters for attention projection and quantization strategies."
      ],
      "metadata": {
        "id": "ZIVJ4KNBAflr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcSILKatOEMB"
      },
      "source": [
        "## MPNET-V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulSECdCwOGtr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# ------------------ Paths ------------------\n",
        "BASE_PATH = \"/workspace\"\n",
        "MODELS_PATH = os.path.join(BASE_PATH, \"models\")\n",
        "CACHE_PATH = os.path.join(BASE_PATH, \"cache\")\n",
        "CHECKPOINTS_PATH = os.path.join(BASE_PATH, \"checkpoints\")\n",
        "\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "os.makedirs(CACHE_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n",
        "\n",
        "os.environ['HF_HOME'] = CACHE_PATH\n",
        "os.environ['TRANSFORMERS_CACHE'] = os.path.join(CACHE_PATH, \"transformers\")\n",
        "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_PATH, \"datasets\")\n",
        "os.environ['TORCH_HOME'] = os.path.join(CACHE_PATH, \"torch\")\n",
        "os.environ['TMPDIR'] = os.path.join(CACHE_PATH, \"tmp\")\n",
        "\n",
        "# ------------------ Force FP32 ------------------\n",
        "os.environ[\"ACCELERATE_DISABLE_FP16\"] = \"1\"  # disables all FP16/BF16\n",
        "os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\"\n",
        "\n",
        "# ------------------ Cleanup ------------------\n",
        "def reset_memory_disk():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    tmp_dirs = [os.environ['TMPDIR'], os.environ['TRANSFORMERS_CACHE']]\n",
        "    for d in tmp_dirs:\n",
        "        if os.path.exists(d):\n",
        "            for f in os.listdir(d):\n",
        "                f_path = os.path.join(d, f)\n",
        "                try:\n",
        "                    if os.path.isfile(f_path) or os.path.islink(f_path):\n",
        "                        os.unlink(f_path)\n",
        "                    elif os.path.isdir(f_path):\n",
        "                        shutil.rmtree(f_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to delete {f_path}: {e}\")\n",
        "    print(\"Memory and disk reset done.\")\n",
        "\n",
        "# ------------------ Dataset Helpers ------------------\n",
        "def prepare_dataset_for_trainer(dataset):\n",
        "    prepared_data = []\n",
        "    for item in dataset:\n",
        "        body = item.get('body') if isinstance(item, dict) else item['body']\n",
        "        pair = item.get('pair') if isinstance(item, dict) else item['pair']\n",
        "        sim = float(item.get('similarity', 0.0) if isinstance(item, dict) else item['similarity'])\n",
        "        prepared_data.append({'sentence1': body, 'sentence2': pair, 'label': sim})\n",
        "    return Dataset.from_list(prepared_data)\n",
        "\n",
        "def convert_to_input_examples(dataset):\n",
        "    examples = []\n",
        "    for item in dataset:\n",
        "        body = item.get('body') if isinstance(item, dict) else item['body']\n",
        "        pair = item.get('pair') if isinstance(item, dict) else item['pair']\n",
        "        sim = float(item.get('similarity', 0.0) if isinstance(item, dict) else item['similarity'])\n",
        "        examples.append(InputExample(texts=[body, pair], label=sim))\n",
        "    return examples\n",
        "\n",
        "# ------------------ Model Loader ------------------\n",
        "def create_sentence_transformer_mpnet():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Loading all-mpnet-base-v2 on device: {device}\")\n",
        "    try:\n",
        "        model = SentenceTransformer(\n",
        "            \"sentence-transformers/all-mpnet-base-v2\",\n",
        "            cache_folder=os.environ['TRANSFORMERS_CACHE'],\n",
        "            device=device\n",
        "        )\n",
        "        print(\"Loaded all-mpnet-base-v2 model.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load all-mpnet-base-v2: {e}, falling back to MiniLM\")\n",
        "        model = SentenceTransformer(\n",
        "            \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            cache_folder=os.environ['TRANSFORMERS_CACHE'],\n",
        "            device=device\n",
        "        )\n",
        "        print(\"Loaded fallback MiniLM model.\")\n",
        "\n",
        "    model.max_seq_length = 512\n",
        "    return model\n",
        "\n",
        "# ------------------ Training Function ------------------\n",
        "def train_mpnet_similarity_model(train_dataset, test_dataset, output_dir):\n",
        "    reset_memory_disk()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model = create_sentence_transformer_mpnet()\n",
        "\n",
        "    train_data = prepare_dataset_for_trainer(train_dataset)\n",
        "    test_data = prepare_dataset_for_trainer(test_dataset)\n",
        "    test_examples = convert_to_input_examples(test_dataset)\n",
        "\n",
        "    train_loss = losses.CosineSimilarityLoss(model)\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(\n",
        "        sentences1=[ex.texts[0] for ex in test_examples],\n",
        "        sentences2=[ex.texts[1] for ex in test_examples],\n",
        "        labels=[float(ex.label) for ex in test_examples],\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    args = SentenceTransformerTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,  # reduced for MPNet memory\n",
        "        gradient_accumulation_steps=8,  # simulate larger batch\n",
        "        learning_rate=2e-5,\n",
        "        warmup_ratio=0.1,\n",
        "        fp16=False,  # FP32 only\n",
        "        bf16=False,\n",
        "        max_grad_norm=1.0,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        save_only_model=True,\n",
        "        logging_steps=50,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        dataloader_num_workers=0,\n",
        "        remove_unused_columns=False,\n",
        "        run_name=\"mpnet-similarity-finetuning\"\n",
        "    )\n",
        "\n",
        "    trainer = SentenceTransformerTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_data,\n",
        "        loss=train_loss,\n",
        "        evaluator=evaluator\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    final_model_path = os.path.join(output_dir, \"final_model\")\n",
        "    model.save(final_model_path)\n",
        "    print(f\"Training complete. Model saved at: {final_model_path}\")\n",
        "    return model, final_model_path\n",
        "\n",
        "# ------------------ Inference Helpers ------------------\n",
        "def load_trained_model(model_path):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = SentenceTransformer(model_path, device=device)\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    return model\n",
        "\n",
        "def compute_similarity(model, text1, text2):\n",
        "    embeddings = model.encode([text1, text2], convert_to_tensor=True)\n",
        "    similarity = torch.nn.functional.cosine_similarity(embeddings[0:1], embeddings[1:2])\n",
        "    return similarity.item()\n",
        "\n",
        "# ------------------ Example Run ------------------\n",
        "if __name__ == \"__main__\":\n",
        "    output_dir = os.path.join(CHECKPOINTS_PATH, \"mpnet_similarity_model\")\n",
        "    try:\n",
        "\n",
        "        train_dataset_s, _ = sample_dataset_stratified(train_dataset, sample_fraction=0.1)\n",
        "        test_dataset_s, _  = sample_dataset_stratified(test_dataset, sample_fraction=0.1)\n",
        "\n",
        "        model, model_path = train_mpnet_similarity_model(\n",
        "            train_dataset_s[\"train\"],\n",
        "            test_dataset_s[\"train\"],\n",
        "            output_dir\n",
        "        )\n",
        "\n",
        "        # Example inference\n",
        "        embeddings = model.encode([\"This is a test sentence.\"], convert_to_tensor=True)\n",
        "        print(\"Example embedding:\", embeddings)\n",
        "\n",
        "        sim_score = compute_similarity(model, \"This is a test sentence.\", \"Another test sentence.\")\n",
        "        print(\"Similarity score:\", sim_score)\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Please define your train_dataset, test_dataset, and sample_dataset_stratified function before running this example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FPDxNlvORzV"
      },
      "source": [
        "## LLAMA-3 (8 billion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "YVKwT-vrUCYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtFtmZWjOUDV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.models import Transformer, Pooling\n",
        "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# New imports\n",
        "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# ------------------ RunPod Network Paths ------------------\n",
        "BASE_PATH = \"/workspace\"\n",
        "MODELS_PATH = os.path.join(BASE_PATH, \"models\")\n",
        "CACHE_PATH = os.path.join(BASE_PATH, \"cache\")\n",
        "CHECKPOINTS_PATH = os.path.join(BASE_PATH, \"checkpoints\")\n",
        "\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "os.makedirs(CACHE_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n",
        "\n",
        "# Force all caches to network volume\n",
        "os.environ['HF_HOME'] = CACHE_PATH\n",
        "os.environ['TRANSFORMERS_CACHE'] = os.path.join(CACHE_PATH, \"transformers\")\n",
        "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_PATH, \"datasets\")\n",
        "os.environ['TORCH_HOME'] = os.path.join(CACHE_PATH, \"torch\")\n",
        "os.environ['TMPDIR'] = os.path.join(CACHE_PATH, \"tmp\")\n",
        "\n",
        "# ------------------ Memory / Disk Cleanup ------------------\n",
        "def reset_memory_disk():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    tmp_dirs = [os.environ['TMPDIR'], os.environ['TRANSFORMERS_CACHE']]\n",
        "    for d in tmp_dirs:\n",
        "        if os.path.exists(d):\n",
        "            for f in os.listdir(d):\n",
        "                f_path = os.path.join(d, f)\n",
        "                try:\n",
        "                    if os.path.isfile(f_path) or os.path.islink(f_path):\n",
        "                        os.unlink(f_path)\n",
        "                    elif os.path.isdir(f_path):\n",
        "                        shutil.rmtree(f_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to delete {f_path}: {e}\")\n",
        "    print(\"Memory and disk reset done.\")\n",
        "\n",
        "# ------------------ Dataset Preparation ------------------\n",
        "def prepare_dataset_for_trainer(dataset):\n",
        "    prepared_data = []\n",
        "    for item in dataset:\n",
        "        body = item.get('body') if isinstance(item, dict) else item['body']\n",
        "        pair = item.get('pair') if isinstance(item, dict) else item['pair']\n",
        "        sim = float(item.get('similarity', 0.0) if isinstance(item, dict) else item['similarity'])\n",
        "        prepared_data.append({'sentence1': body, 'sentence2': pair, 'label': sim})\n",
        "    return Dataset.from_list(prepared_data)\n",
        "\n",
        "def convert_to_input_examples(dataset):\n",
        "    examples = []\n",
        "    for item in dataset:\n",
        "        body = item.get('body') if isinstance(item, dict) else item['body']\n",
        "        pair = item.get('pair') if isinstance(item, dict) else item['pair']\n",
        "        sim = float(item.get('similarity', 0.0) if isinstance(item, dict) else item['similarity'])\n",
        "        examples.append(InputExample(texts=[body, pair], label=sim))\n",
        "    return examples\n",
        "\n",
        "# ------------------ LLaMA-based SentenceTransformer ------------------\n",
        "def create_sentence_transformer_from_llama(hf_model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Loading LLaMA model on device: {device}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(hf_model_id, cache_dir=os.environ['TRANSFORMERS_CACHE'])\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Try 4-bit first, fallback to BF16\n",
        "    try:\n",
        "        print(\"Attempting 4-bit quantized load...\")\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        base_model = AutoModel.from_pretrained(\n",
        "            hf_model_id,\n",
        "            cache_dir=os.environ['TRANSFORMERS_CACHE'],\n",
        "            quantization_config=quant_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(\"Loaded model in 4-bit mode.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  4-bit load failed: {e}\")\n",
        "        print(\"Falling back to BF16 full precision...\")\n",
        "        base_model = AutoModel.from_pretrained(\n",
        "            hf_model_id,\n",
        "            cache_dir=os.environ['TRANSFORMERS_CACHE'],\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    # Enable gradient checkpointing to save VRAM\n",
        "    base_model.gradient_checkpointing_enable()\n",
        "\n",
        "    # LoRA config for efficient fine-tuning\n",
        "    lora_config = LoraConfig(r=32, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.1, bias=\"none\", task_type=\"FEATURE_EXTRACTION\")\n",
        "    base_model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "    # Wrap in sentence-transformers\n",
        "    word_embedding_model = Transformer(\n",
        "        model_name_or_path=hf_model_id,\n",
        "        max_seq_length=256,   # reduced for speed/memory\n",
        "        cache_dir=os.environ['TRANSFORMERS_CACHE']\n",
        "    )\n",
        "    word_embedding_model.tokenizer = tokenizer\n",
        "    word_embedding_model.auto_model = base_model\n",
        "\n",
        "    pooling_model = Pooling(\n",
        "        word_embedding_model.get_word_embedding_dimension(),\n",
        "        pooling_mode_mean_tokens=True,\n",
        "        pooling_mode_cls_token=False,\n",
        "        pooling_mode_max_tokens=False\n",
        "    )\n",
        "\n",
        "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device=device)\n",
        "    print(\"✅ LLaMA-based SentenceTransformer initialized (LoRA + checkpointing).\")\n",
        "    return model\n",
        "\n",
        "# ------------------ Training Function ------------------\n",
        "def train_llama_similarity_model(train_dataset, test_dataset, output_dir):\n",
        "    reset_memory_disk()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model = create_sentence_transformer_from_llama()\n",
        "\n",
        "    train_data = prepare_dataset_for_trainer(train_dataset)\n",
        "    test_data = prepare_dataset_for_trainer(test_dataset)\n",
        "    test_examples = convert_to_input_examples(test_dataset)\n",
        "\n",
        "    train_loss = losses.CosineSimilarityLoss(model)\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(\n",
        "        sentences1=[ex.texts[0] for ex in test_examples],\n",
        "        sentences2=[ex.texts[1] for ex in test_examples],\n",
        "        labels=[float(ex.label) for ex in test_examples],\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    args = SentenceTransformerTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "\n",
        "        learning_rate=2e-5,\n",
        "        warmup_ratio=0.1,\n",
        "        bf16=True,                       # better on B200\n",
        "        fp16=False,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        save_total_limit=1,\n",
        "        save_only_model=True,\n",
        "        logging_steps=100,\n",
        "        remove_unused_columns=False,\n",
        "        run_name=\"llama-similarity-finetuning\"\n",
        "    )\n",
        "\n",
        "    trainer = SentenceTransformerTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_data,\n",
        "        loss=train_loss,\n",
        "        evaluator=evaluator\n",
        "    )\n",
        "\n",
        "    print(\"🚀 Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    final_model_path = os.path.join(output_dir, \"final_model\")\n",
        "    model.save(final_model_path)\n",
        "    print(f\"🎉 Training complete. Model saved at: {final_model_path}\")\n",
        "    return model, final_model_path\n",
        "\n",
        "# ------------------ Example Run ------------------\n",
        "if __name__ == \"__main__\":\n",
        "    output_dir = os.path.join(CHECKPOINTS_PATH, \"llama_similarity_model\")\n",
        "    train_dataset_s, _ = sample_dataset_stratified(train_dataset, sample_fraction=0.1)\n",
        "    test_dataset_s, _  = sample_dataset_stratified(test_dataset, sample_fraction=0.1)\n",
        "    model, model_path = train_llama_similarity_model(train_dataset_s[\"train\"], test_dataset_s[\"train\"], output_dir)\n",
        "\n",
        "    embeddings = model.encode([\"This is a test sentence.\"], convert_to_tensor=True)\n",
        "    print(\"Example embedding:\", embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1rXsHdEPdkw"
      },
      "source": [
        "## Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41xeaF23QThG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.models import Transformer, Pooling\n",
        "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# New imports\n",
        "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# ------------------ RunPod Network Paths ------------------\n",
        "BASE_PATH = \"/workspace\"\n",
        "MODELS_PATH = os.path.join(BASE_PATH, \"models\")\n",
        "CACHE_PATH = os.path.join(BASE_PATH, \"cache\")\n",
        "CHECKPOINTS_PATH = os.path.join(BASE_PATH, \"checkpoints\")\n",
        "\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "os.makedirs(CACHE_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n",
        "\n",
        "# Force all caches to network volume\n",
        "os.environ['HF_HOME'] = CACHE_PATH\n",
        "os.environ['TRANSFORMERS_CACHE'] = os.path.join(CACHE_PATH, \"transformers\")\n",
        "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_PATH, \"datasets\")\n",
        "os.environ['TORCH_HOME'] = os.path.join(CACHE_PATH, \"torch\")\n",
        "os.environ['TMPDIR'] = os.path.join(CACHE_PATH, \"tmp\")\n",
        "\n",
        "# ------------------ Memory / Disk Cleanup ------------------\n",
        "def reset_memory_disk():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    tmp_dirs = [os.environ['TMPDIR'], os.environ['TRANSFORMERS_CACHE']]\n",
        "    for d in tmp_dirs:\n",
        "        if os.path.exists(d):\n",
        "            for f in os.listdir(d):\n",
        "                f_path = os.path.join(d, f)\n",
        "                try:\n",
        "                    if os.path.isfile(f_path) or os.path.islink(f_path):\n",
        "                        os.unlink(f_path)\n",
        "                    elif os.path.isdir(f_path):\n",
        "                        shutil.rmtree(f_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to delete {f_path}: {e}\")\n",
        "    print(\"Memory and disk reset done.\")\n",
        "\n",
        "# ------------------ Dataset Preparation ------------------\n",
        "def prepare_dataset_for_trainer(dataset):\n",
        "    prepared_data = []\n",
        "    for item in dataset:\n",
        "        body = item.get('body') if isinstance(item, dict) else item['body']\n",
        "        pair = item.get('pair') if isinstance(item, dict) else item['pair']\n",
        "        sim = float(item.get('similarity', 0.0) if isinstance(item, dict) else item['similarity'])\n",
        "        prepared_data.append({'sentence1': body, 'sentence2': pair, 'label': sim})\n",
        "    return Dataset.from_list(prepared_data)\n",
        "\n",
        "def convert_to_input_examples(dataset):\n",
        "    examples = []\n",
        "    for item in dataset:\n",
        "        body = item.get('body') if isinstance(item, dict) else item['body']\n",
        "        pair = item.get('pair') if isinstance(item, dict) else item['pair']\n",
        "        sim = float(item.get('similarity', 0.0) if isinstance(item, dict) else item['similarity'])\n",
        "        examples.append(InputExample(texts=[body, pair], label=sim))\n",
        "    return examples\n",
        "\n",
        "# ------------------ Mistral-based SentenceTransformer ------------------\n",
        "def create_sentence_transformer_from_mistral(hf_model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Loading Mistral model on device: {device}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(hf_model_id, cache_dir=os.environ['TRANSFORMERS_CACHE'])\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Try 4-bit first, fallback to BF16\n",
        "    try:\n",
        "        print(\"Attempting 4-bit quantized load...\")\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        base_model = AutoModel.from_pretrained(\n",
        "            hf_model_id,\n",
        "            cache_dir=os.environ['TRANSFORMERS_CACHE'],\n",
        "            quantization_config=quant_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(\"Loaded model in 4-bit mode.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  4-bit load failed: {e}\")\n",
        "        print(\"Falling back to BF16 full precision...\")\n",
        "        base_model = AutoModel.from_pretrained(\n",
        "            hf_model_id,\n",
        "            cache_dir=os.environ['TRANSFORMERS_CACHE'],\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    # Enable gradient checkpointing\n",
        "    base_model.gradient_checkpointing_enable()\n",
        "\n",
        "    # LoRA config tuned for Mistral\n",
        "    lora_config = LoraConfig(\n",
        "        r=32,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"FEATURE_EXTRACTION\"\n",
        "    )\n",
        "    base_model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "    # Wrap in sentence-transformers\n",
        "    word_embedding_model = Transformer(\n",
        "        model_name_or_path=hf_model_id,\n",
        "        max_seq_length=256,   # reduced for speed/memory\n",
        "        cache_dir=os.environ['TRANSFORMERS_CACHE']\n",
        "    )\n",
        "    word_embedding_model.tokenizer = tokenizer\n",
        "    word_embedding_model.auto_model = base_model\n",
        "\n",
        "    pooling_model = Pooling(\n",
        "        word_embedding_model.get_word_embedding_dimension(),\n",
        "        pooling_mode_mean_tokens=True,\n",
        "        pooling_mode_cls_token=False,\n",
        "        pooling_mode_max_tokens=False\n",
        "    )\n",
        "\n",
        "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device=device)\n",
        "    print(\"✅ Mistral-based SentenceTransformer initialized (LoRA + checkpointing).\")\n",
        "    return model\n",
        "\n",
        "# ------------------ Training Function ------------------\n",
        "def train_mistral_similarity_model(train_dataset, test_dataset, output_dir):\n",
        "    reset_memory_disk()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model = create_sentence_transformer_from_mistral()\n",
        "\n",
        "    train_data = prepare_dataset_for_trainer(train_dataset)\n",
        "    test_data = prepare_dataset_for_trainer(test_dataset)\n",
        "    test_examples = convert_to_input_examples(test_dataset)\n",
        "\n",
        "    train_loss = losses.CosineSimilarityLoss(model)\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(\n",
        "        sentences1=[ex.texts[0] for ex in test_examples],\n",
        "        sentences2=[ex.texts[1] for ex in test_examples],\n",
        "        labels=[float(ex.label) for ex in test_examples],\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    args = SentenceTransformerTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-5,\n",
        "        warmup_ratio=0.1,\n",
        "        bf16=True,   # good for B200\n",
        "        fp16=False,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        save_total_limit=1,\n",
        "        save_only_model=True,\n",
        "        logging_steps=100,\n",
        "        remove_unused_columns=False,\n",
        "        run_name=\"mistral-similarity-finetuning\"\n",
        "    )\n",
        "\n",
        "    trainer = SentenceTransformerTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_data,\n",
        "        loss=train_loss,\n",
        "        evaluator=evaluator\n",
        "    )\n",
        "\n",
        "    print(\"🚀 Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    final_model_path = os.path.join(output_dir, \"final_model\")\n",
        "    model.save(final_model_path)\n",
        "    print(f\"🎉 Training complete. Model saved at: {final_model_path}\")\n",
        "    return model, final_model_path\n",
        "\n",
        "# ------------------ Example Run ------------------\n",
        "if __name__ == \"__main__\":\n",
        "    output_dir = os.path.join(CHECKPOINTS_PATH, \"mistral_similarity_model\")\n",
        "\n",
        "    # Use your already-defined function\n",
        "    train_dataset_s, _ = sample_dataset_stratified(train_dataset, sample_fraction=0.1)\n",
        "    test_dataset_s, _  = sample_dataset_stratified(test_dataset, sample_fraction=0.1)\n",
        "\n",
        "    model, model_path = train_mistral_similarity_model(\n",
        "        train_dataset_s[\"train\"],\n",
        "        test_dataset_s[\"train\"],\n",
        "        output_dir\n",
        "    )\n",
        "\n",
        "    embeddings = model.encode([\"This is a test sentence.\"], convert_to_tensor=True)\n",
        "    print(\"Example embedding:\", embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1UfiUlPUinu"
      },
      "source": [
        "## QWEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtdAH0heUk1k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.models import Transformer, Pooling\n",
        "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# New imports\n",
        "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# ------------------ RunPod Network Paths ------------------\n",
        "BASE_PATH = \"/workspace\"\n",
        "MODELS_PATH = os.path.join(BASE_PATH, \"models\")\n",
        "CACHE_PATH = os.path.join(BASE_PATH, \"cache\")\n",
        "CHECKPOINTS_PATH = os.path.join(BASE_PATH, \"checkpoints\")\n",
        "\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "os.makedirs(CACHE_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n",
        "\n",
        "# Force all caches to network volume\n",
        "os.environ['HF_HOME'] = CACHE_PATH\n",
        "os.environ['TRANSFORMERS_CACHE'] = os.path.join(CACHE_PATH, \"transformers\")\n",
        "os.environ['HF_DATASETS_CACHE'] = os.path.join(CACHE_PATH, \"datasets\")\n",
        "os.environ['TORCH_HOME'] = os.path.join(CACHE_PATH, \"torch\")\n",
        "os.environ['TMPDIR'] = os.path.join(CACHE_PATH, \"tmp\")\n",
        "\n",
        "# ------------------ Memory / Disk Cleanup ------------------\n",
        "def reset_memory_disk():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    tmp_dirs = [os.environ['TMPDIR'], os.environ['TRANSFORMERS_CACHE']]\n",
        "    for d in tmp_dirs:\n",
        "        if os.path.exists(d):\n",
        "            for f in os.listdir(d):\n",
        "                f_path = os.path.join(d, f)\n",
        "                try:\n",
        "                    if os.path.isfile(f_path) or os.path.islink(f_path):\n",
        "                        os.unlink(f_path)\n",
        "                    elif os.path.isdir(f_path):\n",
        "                        shutil.rmtree(f_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to delete {f_path}: {e}\")\n",
        "    print(\"Memory and disk reset done.\")\n",
        "\n",
        "# ------------------ Dataset Preparation ------------------\n",
        "def prepare_dataset_for_trainer(dataset):\n",
        "    prepared_data = []\n",
        "    for item in dataset:\n",
        "        body = item.get('body') if isinstance(item, dict) else item['body']\n",
        "        pair = item.get('pair') if isinstance(item, dict) else item['pair']\n",
        "        sim = float(item.get('similarity', 0.0) if isinstance(item, dict) else item['similarity'])\n",
        "        prepared_data.append({'sentence1': body, 'sentence2': pair, 'label': sim})\n",
        "    return Dataset.from_list(prepared_data)\n",
        "\n",
        "def convert_to_input_examples(dataset):\n",
        "    examples = []\n",
        "    for item in dataset:\n",
        "        body = item.get('body') if isinstance(item, dict) else item['body']\n",
        "        pair = item.get('pair') if isinstance(item, dict) else item['pair']\n",
        "        sim = float(item.get('similarity', 0.0) if isinstance(item, dict) else item['similarity'])\n",
        "        examples.append(InputExample(texts=[body, pair], label=sim))\n",
        "    return examples\n",
        "\n",
        "# ------------------ Qwen-based SentenceTransformer ------------------\n",
        "def create_sentence_transformer_from_qwen(hf_model_id=\"Qwen/Qwen2-7B-Instruct\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Loading Qwen model on device: {device}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(hf_model_id, cache_dir=os.environ['TRANSFORMERS_CACHE'])\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Try 4-bit first, fallback to BF16\n",
        "    try:\n",
        "        print(\"Attempting 4-bit quantized load...\")\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        base_model = AutoModel.from_pretrained(\n",
        "            hf_model_id,\n",
        "            cache_dir=os.environ['TRANSFORMERS_CACHE'],\n",
        "            quantization_config=quant_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(\"Loaded model in 4-bit mode.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  4-bit load failed: {e}\")\n",
        "        print(\"Falling back to BF16 full precision...\")\n",
        "        base_model = AutoModel.from_pretrained(\n",
        "            hf_model_id,\n",
        "            cache_dir=os.environ['TRANSFORMERS_CACHE'],\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    base_model.gradient_checkpointing_enable()\n",
        "\n",
        "    # LoRA config (Qwen uses different attention proj names than LLaMA)\n",
        "    lora_config = LoraConfig(\n",
        "        r=32,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"FEATURE_EXTRACTION\"\n",
        "    )\n",
        "\n",
        "    base_model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "    # Wrap in sentence-transformers\n",
        "    word_embedding_model = Transformer(\n",
        "        model_name_or_path=hf_model_id,\n",
        "        max_seq_length=256,\n",
        "        cache_dir=os.environ['TRANSFORMERS_CACHE']\n",
        "    )\n",
        "    word_embedding_model.tokenizer = tokenizer\n",
        "    word_embedding_model.auto_model = base_model\n",
        "\n",
        "    pooling_model = Pooling(\n",
        "        word_embedding_model.get_word_embedding_dimension(),\n",
        "        pooling_mode_mean_tokens=True,\n",
        "        pooling_mode_cls_token=False,\n",
        "        pooling_mode_max_tokens=False\n",
        "    )\n",
        "\n",
        "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device=device)\n",
        "    print(\"✅ Qwen-based SentenceTransformer initialized (LoRA + checkpointing).\")\n",
        "    return model\n",
        "\n",
        "# ------------------ Training Function ------------------\n",
        "def train_qwen_similarity_model(train_dataset, test_dataset, output_dir):\n",
        "    reset_memory_disk()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model = create_sentence_transformer_from_qwen()\n",
        "\n",
        "    train_data = prepare_dataset_for_trainer(train_dataset)\n",
        "    test_data = prepare_dataset_for_trainer(test_dataset)\n",
        "    test_examples = convert_to_input_examples(test_dataset)\n",
        "\n",
        "    train_loss = losses.CosineSimilarityLoss(model)\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(\n",
        "        sentences1=[ex.texts[0] for ex in test_examples],\n",
        "        sentences2=[ex.texts[1] for ex in test_examples],\n",
        "        labels=[float(ex.label) for ex in test_examples],\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    args = SentenceTransformerTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "\n",
        "        learning_rate=2e-5,\n",
        "        warmup_ratio=0.1,\n",
        "        bf16=True,\n",
        "        fp16=False,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        save_total_limit=1,\n",
        "        save_only_model=True,\n",
        "        logging_steps=100,\n",
        "        remove_unused_columns=False,\n",
        "        run_name=\"qwen-similarity-finetuning\"\n",
        "    )\n",
        "\n",
        "    trainer = SentenceTransformerTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_data,\n",
        "        loss=train_loss,\n",
        "        evaluator=evaluator\n",
        "    )\n",
        "\n",
        "    print(\"🚀 Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    final_model_path = os.path.join(output_dir, \"final_model\")\n",
        "    model.save(final_model_path)\n",
        "    print(f\"🎉 Training complete. Model saved at: {final_model_path}\")\n",
        "    return model, final_model_path\n",
        "\n",
        "# ------------------ Example Run ------------------\n",
        "if __name__ == \"__main__\":\n",
        "    output_dir = os.path.join(CHECKPOINTS_PATH, \"qwen_similarity_model\")\n",
        "    train_dataset_s, _ = sample_dataset_stratified(train_dataset, sample_fraction=0.1)\n",
        "    test_dataset_s, _  = sample_dataset_stratified(test_dataset, sample_fraction=0.1)\n",
        "    model, model_path = train_qwen_similarity_model(train_dataset_s[\"train\"], test_dataset_s[\"train\"], output_dir)\n",
        "\n",
        "    embeddings = model.encode([\"This is a test sentence.\"], convert_to_tensor=True)\n",
        "    print(\"Example embedding:\", embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXVClwBPCBC8"
      },
      "source": [
        "# Embedding Drift Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this section implements a data quality flagging system that uses semantic drift detection between pairs. It encodes text data (bodies and pairs) into embeddings, computes cosine drift scores (1 minus cosine similarity), and uses statistical methods (Gaussian Mixture Models with KDE fallback) to automatically determine an optimal threshold for flagging problematic samples. The system is calibrated to achieve a target flagging rate (50%) while keeping false positives on clean data below 3%."
      ],
      "metadata": {
        "id": "g5z6kPwEAsFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import brentq\n",
        "from scipy.stats import gaussian_kde\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "batch_size = 64\n",
        "target_total_frac = 0.50       # desired overall flagged fraction (50%)\n",
        "clean_fp_target = 0.03         # ≤3% clean false positive\n",
        "kde_grid_points = 2000\n",
        "\n",
        "# ---------------------------\n",
        "# Helper - safe add column\n",
        "# ---------------------------\n",
        "def safe_add_column(split_ds, name, values):\n",
        "    if name in split_ds.column_names:\n",
        "        split_ds = split_ds.remove_columns(name)\n",
        "    return split_ds.add_column(name, values)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Encode prompts\n",
        "# ---------------------------\n",
        "def encode_with_progress(texts, desc=\"Encoding\", batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=desc):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        emb = model.encode(batch, show_progress_bar=False, convert_to_numpy=True, batch_size=batch_size)\n",
        "        embeddings.append(emb)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "print(\"🔄 Encoding dataset...\")\n",
        "bodies = test_dataset[\"train\"][\"body\"]\n",
        "pairs = test_dataset[\"train\"][\"pair\"]\n",
        "\n",
        "body_embs = encode_with_progress(bodies, desc=\"Bodies\", batch_size=batch_size)\n",
        "pair_embs = encode_with_progress(pairs, desc=\"Pairs\", batch_size=batch_size)\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Compute drift scores = 1 - cosine\n",
        "# ---------------------------\n",
        "print(\"🔄 Computing cosine drift (vectorized)...\")\n",
        "A = np.array(body_embs, dtype=np.float32)\n",
        "B = np.array(pair_embs, dtype=np.float32)\n",
        "\n",
        "A_norm = np.linalg.norm(A, axis=1, keepdims=True)\n",
        "B_norm = np.linalg.norm(B, axis=1, keepdims=True)\n",
        "A_norm = np.where(A_norm == 0, 1.0, A_norm)\n",
        "B_norm = np.where(B_norm == 0, 1.0, B_norm)\n",
        "\n",
        "A_unit = A / A_norm\n",
        "B_unit = B / B_norm\n",
        "\n",
        "cosine_scores = np.sum(A_unit * B_unit, axis=1)   # similarity in [-1, 1]\n",
        "drift_scores = 1.0 - cosine_scores                # drift in [0, 2]\n",
        "\n",
        "drift_col_name = \"drift_score\"\n",
        "train_ds = test_dataset[\"train\"]\n",
        "train_ds = safe_add_column(train_ds, drift_col_name, drift_scores.tolist())\n",
        "test_dataset[\"train\"] = train_ds\n",
        "\n",
        "print(f\"✅ Computed and saved '{drift_col_name}' \"\n",
        "      f\"(min={drift_scores.min():.6f}, max={drift_scores.max():.6f})\")\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Threshold determination (GMM primary, KDE fallback)\n",
        "# ---------------------------\n",
        "print(\"🔄 Calibrating thresholds (GMM + KDE fallback)...\")\n",
        "sims = np.asarray(drift_scores, dtype=float)   # now using drift\n",
        "n = len(sims)\n",
        "target_total_flagged = int(target_total_frac * n)\n",
        "\n",
        "def try_gmm_thresholding(vals):\n",
        "    try:\n",
        "        gmm = GaussianMixture(n_components=2, random_state=0, covariance_type='full', n_init=5)\n",
        "        gmm.fit(vals.reshape(-1,1))\n",
        "        means = gmm.means_.flatten()\n",
        "        covs = gmm.covariances_.flatten()\n",
        "        weights = gmm.weights_.flatten()\n",
        "        sigmas = np.sqrt(covs)\n",
        "\n",
        "        clean_comp = int(np.argmin(means))  # lower-mean = clean (low drift)\n",
        "        other_comp = 1 - clean_comp\n",
        "\n",
        "        mu_c = float(means[clean_comp]); sigma_c = float(sigmas[clean_comp]); w_c = float(weights[clean_comp])\n",
        "        mu_o = float(means[other_comp]); sigma_o = float(sigmas[other_comp]); w_o = float(weights[other_comp])\n",
        "\n",
        "        def wpdf_diff(x):\n",
        "            return w_c * norm.pdf(x, loc=mu_c, scale=sigma_c) - w_o * norm.pdf(x, loc=mu_o, scale=sigma_o)\n",
        "\n",
        "        left = min(mu_c, mu_o) - 5 * max(sigma_c, sigma_o)\n",
        "        right = max(mu_c, mu_o) + 5 * max(sigma_c, sigma_o)\n",
        "\n",
        "        intersect = None\n",
        "        a, b = left, right\n",
        "        for _ in range(6):\n",
        "            fa, fb = wpdf_diff(a), wpdf_diff(b)\n",
        "            if fa == 0: intersect = a; break\n",
        "            if fb == 0: intersect = b; break\n",
        "            if fa * fb < 0:\n",
        "                intersect = brentq(wpdf_diff, a, b); break\n",
        "            a -= (b - a); b += (b - a)\n",
        "        if intersect is None:\n",
        "            intersect = float((mu_c + mu_o) / 2.0)\n",
        "\n",
        "        return True, {\n",
        "            \"method\":\"gmm\",\n",
        "            \"gmm\": gmm,\n",
        "            \"mu_c\": mu_c, \"sigma_c\": sigma_c, \"w_c\": w_c,\n",
        "            \"mu_o\": mu_o, \"sigma_o\": sigma_o, \"w_o\": w_o,\n",
        "            \"intersection\": float(intersect)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return False, {\"error\": str(e)}\n",
        "\n",
        "def kde_fallback_thresholding(vals, grid_points=2000):\n",
        "    kde = gaussian_kde(vals)\n",
        "    grid = np.linspace(vals.min(), vals.max(), grid_points)\n",
        "    dens = kde(grid)\n",
        "    peaks, _ = find_peaks(dens)\n",
        "    if len(peaks) == 0:\n",
        "        return True, {\"method\":\"kde\", \"valley\": float(np.median(vals))}\n",
        "    peak_heights = dens[peaks]\n",
        "    sorted_idx = np.argsort(peak_heights)[::-1]\n",
        "    if len(sorted_idx) == 1:\n",
        "        clean_peak = grid[peaks[sorted_idx[0]]]\n",
        "        valley = float((clean_peak + vals.max())/2.0)\n",
        "        return True, {\"method\":\"kde\", \"valley\": valley}\n",
        "    top_two = peaks[sorted_idx[:2]]\n",
        "    peak_positions = grid[top_two]\n",
        "    clean_peak_pos = float(np.min(peak_positions))   # low drift peak\n",
        "    other_peak_pos = float(np.max(peak_positions))\n",
        "    left_idx = int(np.argmin(np.abs(grid - clean_peak_pos)))\n",
        "    right_idx = int(np.argmin(np.abs(grid - other_peak_pos)))\n",
        "    if left_idx > right_idx:\n",
        "        left_idx, right_idx = right_idx, left_idx\n",
        "    valley_region = dens[left_idx:right_idx+1]\n",
        "    if len(valley_region) == 0:\n",
        "        valley_x = float((clean_peak_pos + other_peak_pos) / 2.0)\n",
        "    else:\n",
        "        valley_rel_idx = np.argmin(valley_region)\n",
        "        valley_x = float(grid[left_idx + valley_rel_idx])\n",
        "\n",
        "    # approximate clean normal from samples near clean_peak_pos\n",
        "    window = (vals.max() - vals.min()) * 0.05\n",
        "    samples_near_clean = vals[(vals >= clean_peak_pos - window) & (vals <= clean_peak_pos + window)]\n",
        "    if len(samples_near_clean) < 8:\n",
        "        window = (vals.max() - vals.min()) * 0.10\n",
        "        samples_near_clean = vals[(vals >= clean_peak_pos - window) & (vals <= clean_peak_pos + window)]\n",
        "    if len(samples_near_clean) >= 8:\n",
        "        mu_c_est = float(np.mean(samples_near_clean))\n",
        "        sigma_c_est = float(np.std(samples_near_clean, ddof=1) + 1e-8)\n",
        "    else:\n",
        "        lower_q = np.percentile(vals, 25)\n",
        "        samples_near_clean = vals[vals <= lower_q]\n",
        "        mu_c_est = float(np.mean(samples_near_clean))\n",
        "        sigma_c_est = float(np.std(samples_near_clean, ddof=1) + 1e-8)\n",
        "\n",
        "    return True, {\"method\":\"kde\",\"valley\": valley_x, \"mu_c_est\": mu_c_est, \"sigma_c_est\": sigma_c_est, \"kde\": None}\n",
        "\n",
        "# Threshold search\n",
        "ok, res = try_gmm_thresholding(sims)\n",
        "if ok and res[\"method\"] == \"gmm\":\n",
        "    used_method = \"gmm\"\n",
        "    t_intersect = float(res[\"intersection\"])\n",
        "    mu_c = res[\"mu_c\"]; sigma_c = res[\"sigma_c\"]\n",
        "\n",
        "    # Cap: clean FP ≤ target\n",
        "    try:\n",
        "        cap_threshold = float(norm.ppf(1 - clean_fp_target, loc=mu_c, scale=sigma_c))\n",
        "    except Exception:\n",
        "        cap_threshold = t_intersect\n",
        "    cap_threshold = min(max(cap_threshold, sims.min()), sims.max())\n",
        "\n",
        "    if np.sum(sims > cap_threshold) >= target_total_flagged:\n",
        "        low, high = float(cap_threshold), float(sims.max())\n",
        "        best_t = low; best_diff = abs(np.sum(sims > low) - target_total_flagged)\n",
        "        for _ in range(40):\n",
        "            mid = (low + high) / 2.0\n",
        "            cnt = np.sum(sims > mid)\n",
        "            diff = cnt - target_total_flagged\n",
        "            if abs(diff) < best_diff:\n",
        "                best_diff = abs(diff); best_t = mid\n",
        "            if cnt >= target_total_flagged: low = mid\n",
        "            else: high = mid\n",
        "        final_threshold = float(best_t)\n",
        "    else:\n",
        "        final_threshold = float(cap_threshold)\n",
        "else:\n",
        "    ok2, kres = kde_fallback_thresholding(sims, grid_points=kde_grid_points)\n",
        "    if not ok2: raise RuntimeError(\"Thresholding failed (GMM and KDE).\")\n",
        "    used_method = \"kde\"\n",
        "    valley = float(kres[\"valley\"])\n",
        "    mu_c = kres.get(\"mu_c_est\", None); sigma_c = kres.get(\"sigma_c_est\", None)\n",
        "    if mu_c is not None and sigma_c is not None:\n",
        "        try:\n",
        "            cap_threshold = float(norm.ppf(1 - clean_fp_target, loc=mu_c, scale=sigma_c))\n",
        "        except Exception:\n",
        "            cap_threshold = valley\n",
        "    else:\n",
        "        cap_threshold = valley\n",
        "\n",
        "    cap_threshold = min(max(cap_threshold, sims.min()), sims.max())\n",
        "    if np.sum(sims > cap_threshold) >= target_total_flagged:\n",
        "        low, high = float(cap_threshold), float(sims.max())\n",
        "        best_t = low; best_diff = abs(np.sum(sims > low) - target_total_flagged)\n",
        "        for _ in range(40):\n",
        "            mid = (low + high) / 2.0\n",
        "            cnt = np.sum(sims > mid)\n",
        "            diff = cnt - target_total_flagged\n",
        "            if abs(diff) < best_diff: best_diff = abs(diff); best_t = mid\n",
        "            if cnt >= target_total_flagged: low = mid\n",
        "            else: high = mid\n",
        "        final_threshold = float(best_t)\n",
        "    else:\n",
        "        final_threshold = float(cap_threshold)\n",
        "\n",
        "threshold = float(final_threshold)\n",
        "flagged_mask = sims > threshold\n",
        "\n",
        "# Save flagged column\n",
        "train_ds = test_dataset[\"train\"]\n",
        "train_ds = safe_add_column(train_ds, \"flagged\", flagged_mask.tolist())\n",
        "test_dataset[\"train\"] = train_ds\n",
        "\n",
        "# Reporting\n",
        "total_flagged = int(np.sum(flagged_mask))\n",
        "empirical_clean_fp = None\n",
        "if \"category\" in test_dataset[\"train\"].column_names:\n",
        "    is_clean = np.array(test_dataset[\"train\"][\"category\"]) == \"clean\"\n",
        "    if np.sum(is_clean) > 0:\n",
        "        empirical_clean_fp = np.sum(flagged_mask[is_clean]) / np.sum(is_clean)\n",
        "\n",
        "print(\"\\n===== Thresholding Summary (Drift) =====\")\n",
        "print(f\"Method used: {used_method}\")\n",
        "print(f\"Final threshold: {threshold:.6f}\")\n",
        "print(f\"Total flagged: {total_flagged}/{n} ({total_flagged/n:.1%})\")\n",
        "if empirical_clean_fp is not None:\n",
        "    print(f\"Empirical clean FP (labels): {empirical_clean_fp:.2%}\")\n",
        "try:\n",
        "    if used_method == \"gmm\":\n",
        "        est_clean_fp = 1 - float(norm.cdf(threshold, loc=mu_c, scale=sigma_c))\n",
        "        print(f\"Estimated clean FP (GMM clean tail): {est_clean_fp:.2%}\")\n",
        "    elif used_method == \"kde\":\n",
        "        if mu_c is not None and sigma_c is not None:\n",
        "            est_clean_fp = 1 - float(norm.cdf(threshold, loc=mu_c, scale=sigma_c))\n",
        "            print(f\"Estimated clean FP (KDE-derived normal tail): {est_clean_fp:.2%}\")\n",
        "except Exception:\n",
        "    pass\n",
        "print(\"================================\\n\")\n",
        "\n",
        "# Category distribution\n",
        "def category_distribution(dataset):\n",
        "    if \"category\" not in dataset[\"train\"].column_names:\n",
        "        print(\"No 'category' column for distribution reporting.\"); return\n",
        "    categories = np.array(dataset[\"train\"][\"category\"])\n",
        "    flagged_arr = np.array(dataset[\"train\"][\"flagged\"])\n",
        "    unique_categories = np.unique(categories)\n",
        "    print(\"\\n📊 Category distribution of flagged samples:\")\n",
        "    for cat in unique_categories:\n",
        "        mask = categories == cat\n",
        "        total = np.sum(mask)\n",
        "        flagged_count = np.sum(flagged_arr[mask])\n",
        "        print(f\"{cat}: {flagged_count}/{total} flagged ({flagged_count/total:.1%})\")\n",
        "\n",
        "category_distribution(test_dataset)"
      ],
      "metadata": {
        "id": "YVd8WGuANWsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation Testing"
      ],
      "metadata": {
        "id": "lF2a1Ji4aqDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.stats import norm, gaussian_kde\n",
        "from scipy.optimize import brentq\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "batch_size = 64\n",
        "target_total_frac = 0.50\n",
        "plot_results = True\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers\n",
        "# ---------------------------\n",
        "def safe_add_column(split_ds, name, values):\n",
        "    if name in split_ds.column_names:\n",
        "        split_ds = split_ds.remove_columns(name)\n",
        "    return split_ds.add_column(name, values)\n",
        "\n",
        "def encode_with_progress(texts, desc=\"Encoding\", batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=desc):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        emb = model.encode(batch, show_progress_bar=False,\n",
        "                           convert_to_numpy=True, batch_size=batch_size)\n",
        "        embeddings.append(emb)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "def compute_scores(A, B, mode=\"cosine_drift\"):\n",
        "    \"\"\"Different drift/similarity scoring functions.\"\"\"\n",
        "    A = np.array(A, dtype=np.float32)\n",
        "    B = np.array(B, dtype=np.float32)\n",
        "    A_norm = np.linalg.norm(A, axis=1, keepdims=True)\n",
        "    B_norm = np.linalg.norm(B, axis=1, keepdims=True)\n",
        "    A_unit = A / np.where(A_norm == 0, 1.0, A_norm)\n",
        "    B_unit = B / np.where(B_norm == 0, 1.0, B_norm)\n",
        "    cosine = np.sum(A_unit * B_unit, axis=1)\n",
        "\n",
        "    if mode == \"cosine_drift\":   # your original\n",
        "        return 1.0 - cosine\n",
        "    elif mode == \"cosine_similarity\":\n",
        "        return cosine\n",
        "    elif mode == \"euclidean\":\n",
        "        return np.linalg.norm(A - B, axis=1)\n",
        "    elif mode == \"angular\":\n",
        "        return np.arccos(np.clip(cosine, -1.0, 1.0))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "def kde_fallback_thresholding(vals, grid_points=2000):\n",
        "    kde = gaussian_kde(vals)\n",
        "    grid = np.linspace(vals.min(), vals.max(), grid_points)\n",
        "    dens = kde(grid)\n",
        "    peaks, _ = find_peaks(dens)\n",
        "    if len(peaks) < 2:\n",
        "        return np.median(vals)\n",
        "    top_two = peaks[np.argsort(dens[peaks])[-2:]]\n",
        "    left, right = np.sort(grid[top_two])\n",
        "    valley_region = dens[(grid >= left) & (grid <= right)]\n",
        "    valley_x = grid[(grid >= left) & (grid <= right)][np.argmin(valley_region)]\n",
        "    return float(valley_x)\n",
        "\n",
        "def gmm_threshold(vals):\n",
        "    gmm = GaussianMixture(n_components=2, random_state=0, covariance_type=\"full\", n_init=5)\n",
        "    gmm.fit(vals.reshape(-1, 1))\n",
        "    means = gmm.means_.flatten()\n",
        "    covs = gmm.covariances_.flatten()\n",
        "    sigmas = np.sqrt(covs)\n",
        "    weights = gmm.weights_.flatten()\n",
        "    clean_comp = np.argmin(means)\n",
        "    other_comp = 1 - clean_comp\n",
        "    mu_c, sigma_c, w_c = means[clean_comp], sigmas[clean_comp], weights[clean_comp]\n",
        "    mu_o, sigma_o, w_o = means[other_comp], sigmas[other_comp], weights[other_comp]\n",
        "\n",
        "    def wpdf_diff(x):\n",
        "        return w_c * norm.pdf(x, mu_c, sigma_c) - w_o * norm.pdf(x, mu_o, sigma_o)\n",
        "\n",
        "    left = min(mu_c, mu_o) - 5 * max(sigma_c, sigma_o)\n",
        "    right = max(mu_c, mu_o) + 5 * max(sigma_c, sigma_o)\n",
        "    try:\n",
        "        intersect = brentq(wpdf_diff, left, right)\n",
        "    except Exception:\n",
        "        intersect = (mu_c + mu_o) / 2.0\n",
        "    return float(intersect), mu_c, sigma_c\n",
        "\n",
        "def adjust_threshold(sims, threshold, target_frac):\n",
        "    n = len(sims)\n",
        "    target_total_flagged = int(target_frac * n)\n",
        "    if np.sum(sims > threshold) >= target_total_flagged:\n",
        "        low, high = threshold, sims.max()\n",
        "        best_t, best_diff = low, abs(np.sum(sims > low) - target_total_flagged)\n",
        "        for _ in range(40):\n",
        "            mid = (low + high) / 2.0\n",
        "            cnt = np.sum(sims > mid)\n",
        "            diff = cnt - target_total_flagged\n",
        "            if abs(diff) < best_diff:\n",
        "                best_diff, best_t = abs(diff), mid\n",
        "            if cnt >= target_total_flagged: low = mid\n",
        "            else: high = mid\n",
        "        return float(best_t)\n",
        "    else:\n",
        "        return float(threshold)\n",
        "\n",
        "def category_distribution(dataset, flagged_arr, label):\n",
        "    if \"category\" not in dataset[\"train\"].column_names:\n",
        "        return\n",
        "    categories = np.array(dataset[\"train\"][\"category\"])\n",
        "    print(f\"\\n📊 Category distribution for {label}:\")\n",
        "    for cat in np.unique(categories):\n",
        "        mask = categories == cat\n",
        "        total = np.sum(mask)\n",
        "        flagged_count = np.sum(flagged_arr[mask])\n",
        "        print(f\"  {cat}: {flagged_count}/{total} flagged ({flagged_count/total:.1%})\")\n",
        "\n",
        "# ---------------------------\n",
        "# Main ablation loop\n",
        "# ---------------------------\n",
        "print(\"🔄 Encoding dataset...\")\n",
        "bodies = test_dataset[\"train\"][\"body\"]\n",
        "pairs = test_dataset[\"train\"][\"pair\"]\n",
        "\n",
        "body_embs = encode_with_progress(bodies, \"Bodies\", batch_size=batch_size)\n",
        "pair_embs = encode_with_progress(pairs, \"Pairs\", batch_size=batch_size)\n",
        "\n",
        "scoring_modes = [\"cosine_drift\", \"cosine_similarity\", \"euclidean\", \"angular\"]\n",
        "threshold_methods = [\"gmm\", \"kde\", \"quantile\", \"fp_cap\"]\n",
        "fp_caps = [0.05, 0.10]   # << only 5% and 10%\n",
        "\n",
        "results = []\n",
        "\n",
        "for score_mode in scoring_modes:\n",
        "    sims = compute_scores(body_embs, pair_embs, mode=score_mode)\n",
        "    n = len(sims)\n",
        "\n",
        "    for method in threshold_methods:\n",
        "        for fp_cap in fp_caps:\n",
        "            label = f\"{score_mode}-{method}-fp{int(fp_cap*100)}\"\n",
        "            try:\n",
        "                if method == \"gmm\":\n",
        "                    thr, mu_c, sigma_c = gmm_threshold(sims)\n",
        "                    cap_thr = norm.ppf(1 - fp_cap, loc=mu_c, scale=sigma_c)\n",
        "                    thr = adjust_threshold(sims, min(cap_thr, sims.max()), target_total_frac)\n",
        "                elif method == \"kde\":\n",
        "                    thr = adjust_threshold(sims, kde_fallback_thresholding(sims), target_total_frac)\n",
        "                elif method == \"quantile\":\n",
        "                    thr = np.quantile(sims, 1 - target_total_frac)\n",
        "                elif method == \"fp_cap\":\n",
        "                    mu, sigma = np.mean(sims), np.std(sims)\n",
        "                    thr = norm.ppf(1 - fp_cap, loc=mu, scale=sigma)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                flagged = sims > thr\n",
        "                frac_flagged = np.mean(flagged)\n",
        "                emp_fp = None\n",
        "                if \"category\" in test_dataset[\"train\"].column_names:\n",
        "                    is_clean = np.array(test_dataset[\"train\"][\"category\"]) == \"clean\"\n",
        "                    if np.sum(is_clean) > 0:\n",
        "                        emp_fp = np.sum(flagged[is_clean]) / np.sum(is_clean)\n",
        "\n",
        "                results.append({\n",
        "                    \"label\": label,\n",
        "                    \"threshold\": thr,\n",
        "                    \"frac_flagged\": frac_flagged,\n",
        "                    \"empirical_fp\": emp_fp\n",
        "                })\n",
        "\n",
        "                print(f\"\\n=== {label} ===\")\n",
        "                print(f\"  Threshold = {thr:.4f}\")\n",
        "                print(f\"  Flagged = {frac_flagged:.1%}\")\n",
        "                if emp_fp is not None:\n",
        "                    print(f\"  Empirical clean FP = {emp_fp:.2%}\")\n",
        "                category_distribution(test_dataset, flagged, label)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ {label} failed: {e}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Plotting\n",
        "# ---------------------------\n",
        "if plot_results:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sims = compute_scores(body_embs, pair_embs, \"cosine_drift\")\n",
        "    plt.hist(sims, bins=200, density=True, alpha=0.35, label=\"hist(cosine_drift)\")\n",
        "    for res in results[:12]:  # show first few thresholds only\n",
        "        plt.axvline(res[\"threshold\"], linestyle=\"--\", linewidth=1.2, label=res[\"label\"])\n",
        "    plt.xlabel(\"score\")\n",
        "    plt.ylabel(\"density\")\n",
        "    plt.title(\"Ablation thresholds (first 12 shown)\")\n",
        "    plt.legend(fontsize=8)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-OD6q9l-hKF-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}